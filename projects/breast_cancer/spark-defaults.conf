#spark.eventLog.dir=/var/logs/spark-history-server
#spark.eventLog.enabled=true
spark.history.kerberos.keytab=none
spark.history.kerberos.principal=none
spark.history.ui.port=18080

spark.driver.extraJavaOptions=-Diop.version=4.2.0.0
spark.yarn.am.extraJavaOptions=-Diop.version=4.2.0.0
spark.yarn.applicationMaster.waitTries=10
spark.yarn.containerLauncherMaxThreads=25
spark.yarn.driver.memoryOverhead=384
#spark.yarn.executor.memoryOverhead=384
spark.yarn.max.executor.failures=3
spark.yarn.preserve.staging.files=false
spark.yarn.queue=default
spark.yarn.scheduler.heartbeat.interval-ms=5000
spark.yarn.submit.file.replication=3

# 15 executors/node * 3 cores/executor -> 45 cores/node
# 120GB/node / 15 executors/node = 8GB/executor
# Save half memory for Python = 4GB/executor
#spark.executor.instances 105
#spark.executor.memory 4g
##spark.executor.memory 10g
#spark.executor.cores 3
##spark.default.parallelism 20000

# Yarn
#spark.executor.instances 9
#spark.executor.memory 90g
#spark.executor.cores 35
#spark.yarn.executor.memoryOverhead 92160
### 3 GPU nodes
#spark.executor.instances 5
#spark.executor.memory 90g
#spark.executor.cores 35
#spark.yarn.executor.memoryOverhead 25600
### Dynamic Allocation
spark.dynamicAllocation.enabled true
spark.dynamicAllocation.initialExecutors 4
spark.dynamicAllocation.minExecutors 1
#spark.dynamicAllocation.maxExecutors 24
#spark.dynamicAllocation.executorIdleTimeout 60s
#spark.dynamicAllocation.cachedExecutorIdleTimeout 60s
#spark.dynamicAllocation.schedulerBacklogTimeout 1s
#spark.dynamicAllocation.sustainedSchedulerBacklogTimeout 1s
#spark.streaming.dynamicAllocation.enabled true
#spark.streaming.dynamicAllocation.scalingInterval 60s
#spark.streaming.dynamicAllocation.scalingUpRatio 0.9
#spark.streaming.dynamicAllocation.scalingDownRatio 0.3
spark.shuffle.service.enabled true
#spark.executor.cores 35
#spark.executor.memory 90g
#spark.yarn.executor.memoryOverhead 25600
spark.executor.cores 4
spark.executor.memory 20g
spark.yarn.executor.memoryOverhead 6144

# Preprocessing - save memory for Python
#spark.executor.memory 51g
#spark.executor.cores 40

# ML
#spark.executor.memory 100g

# Other settings
# TODO: Change to ~100g to allow for Python memory too
spark.driver.memory 200g
spark.driver.extraJavaOptions -server -Xmn12G
spark.executor.extraJavaOptions -server -Xmn12G -XX:+UseG1GC
spark.driver.maxResultSize 0
spark.rpc.message.maxSize 128
spark.network.timeout 1000s
spark.serializer org.apache.spark.serializer.KryoSerializer
spark.kryoserializer.buffer.max 1g

