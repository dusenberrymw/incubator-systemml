{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Breast Cancer Proliferation Scores with Apache Spark and Apache SystemML\n",
    "\n",
    "## Machine Learning\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pyspark.sql.functions import col, max\n",
    "import systemml  # pip3 install systemml\n",
    "from systemml import MLContext, dml\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml = MLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in train & val data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Settings\n",
    "size=256\n",
    "grayscale = False\n",
    "c = 1 if grayscale else 3\n",
    "p = 0.01\n",
    "folder = \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'Path does not exist: file:/Users/mwdusenb/Documents/Code/systemML/breast_cancer/data/train_0.01_sample_256.parquet;'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/Users/mwdusenb/Documents/Code/scala/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mwdusenb/Documents/Code/scala/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o51.load.\n: org.apache.spark.sql.AnalysisException: Path does not exist: file:/Users/mwdusenb/Documents/Code/systemML/breast_cancer/data/train_0.01_sample_256.parquet;\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$14.apply(DataSource.scala:354)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$14.apply(DataSource.scala:342)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.immutable.List.foreach(List.scala:381)\n\tat scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\n\tat scala.collection.immutable.List.flatMap(List.scala:344)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:342)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:178)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:156)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-5dc2dd6523a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mtr_filename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"train_{}{}.parquet\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_grayscale\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mgrayscale\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mval_filename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"val_{}{}.parquet\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_grayscale\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mgrayscale\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtrain_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mval_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mwdusenb/Documents/Code/scala/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mwdusenb/Documents/Code/scala/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mwdusenb/Documents/Code/scala/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'Path does not exist: file:/Users/mwdusenb/Documents/Code/systemML/breast_cancer/data/train_0.01_sample_256.parquet;'"
     ]
    }
   ],
   "source": [
    "if p < 1:\n",
    "  tr_filename = os.path.join(folder, \"train_{}_sample_{}{}.parquet\".format(p, size, \"_grayscale\" if grayscale else \"\"))\n",
    "  val_filename = os.path.join(folder, \"val_{}_sample_{}{}.parquet\".format(p, size, \"_grayscale\" if grayscale else \"\"))\n",
    "else:\n",
    "  tr_filename = os.path.join(folder, \"train_{}{}.parquet\".format(size, \"_grayscale\" if grayscale else \"\"))\n",
    "  val_filename = os.path.join(folder, \"val_{}{}.parquet\".format(size, \"_grayscale\" if grayscale else \"\"))\n",
    "train_df = spark.read.load(tr_filename)\n",
    "val_df = spark.read.load(val_filename)\n",
    "train_df, val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc = train_df.count()\n",
    "vc = val_df.count()\n",
    "tc, vc, tc + vc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.select(max(col(\"__INDEX\"))).show()\n",
    "train_df.groupBy(\"tumor_score\").count().show()\n",
    "val_df.groupBy(\"tumor_score\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract X and Y matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Must use the row index column, or X may not\n",
    "# necessarily correspond correctly to Y\n",
    "X_df = train_df.select(\"__INDEX\", \"sample\")\n",
    "X_val_df = val_df.select(\"__INDEX\", \"sample\")\n",
    "y_df = train_df.select(\"__INDEX\", \"tumor_score\")\n",
    "y_val_df = val_df.select(\"__INDEX\", \"tumor_score\")\n",
    "X_df, X_val_df, y_df, y_val_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert to SystemML Matrices\n",
    "Note: This allows for reuse of the matrices on multiple\n",
    "subsequent script invocations with only a single\n",
    "conversion.  Additionally, since the underlying RDDs\n",
    "backing the SystemML matrices are maintained, any\n",
    "caching will also be maintained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script = \"\"\"\n",
    "# Scale images to [-1,1]\n",
    "X = X / 255\n",
    "X_val = X_val / 255\n",
    "X = X * 2 - 1\n",
    "X_val = X_val * 2 - 1\n",
    "\n",
    "# One-hot encode the labels\n",
    "num_tumor_classes = 3\n",
    "n = nrow(y)\n",
    "n_val = nrow(y_val)\n",
    "Y = table(seq(1, n), y, n, num_tumor_classes)\n",
    "Y_val = table(seq(1, n_val), y_val, n_val, num_tumor_classes)\n",
    "\"\"\"\n",
    "outputs = (\"X\", \"X_val\", \"Y\", \"Y_val\")\n",
    "script = dml(script).input(X=X_df, X_val=X_val_df, y=y_df, y_val=y_val_df).output(*outputs)\n",
    "X, X_val, Y, Y_val = ml.execute(script).get(*outputs)\n",
    "X, X_val, Y, Y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trigger Caching (Optional)\n",
    "Note: This will take a while and is not necessary, but doing it\n",
    "once will speed up the training below. Otherwise, the cost of\n",
    "caching will be spread across the first full loop through the\n",
    "data during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# script = \"\"\"\n",
    "# # Trigger conversions and caching\n",
    "# # Note: This may take a while, but will enable faster iteration later\n",
    "# print(sum(X))\n",
    "# print(sum(Y))\n",
    "# print(sum(X_val))\n",
    "# print(sum(Y_val))\n",
    "# \"\"\"\n",
    "# script = dml(script).input(X=X, X_val=X_val, Y=Y, Y_val=Y_val)\n",
    "# ml.execute(script)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Matrices (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# script = \"\"\"\n",
    "# write(X, \"data/X_\"+p+\"_sample_binary\", format=\"binary\")\n",
    "# write(Y, \"data/Y_\"+p+\"_sample_binary\", format=\"binary\")\n",
    "# write(X_val, \"data/X_val_\"+p+\"_sample_binary\", format=\"binary\")\n",
    "# write(Y_val, \"data/Y_val_\"+p+\"_sample_binary\", format=\"binary\")\n",
    "# \"\"\"\n",
    "# script = dml(script).input(X=X, X_val=X_val, Y=Y, Y_val=Y_val, p=p)\n",
    "# ml.execute(script)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity Check: Overfit Small Portion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script = \"\"\"\n",
    "source(\"breastcancer/softmax_clf.dml\") as clf\n",
    "\n",
    "# Hyperparameters & Settings\n",
    "lr = 1e-2  # learning rate\n",
    "mu = 0.9  # momentum\n",
    "decay = 0.999  # learning rate decay constant\n",
    "batch_size = 32\n",
    "epochs = 500\n",
    "log_interval = 1\n",
    "n = 200  # sample size for overfitting sanity check\n",
    "\n",
    "# Train\n",
    "[W, b] = clf::train(X[1:n,], Y[1:n,], X[1:n,], Y[1:n,], lr, mu, decay, batch_size, epochs, log_interval)\n",
    "\"\"\"\n",
    "outputs = (\"W\", \"b\")\n",
    "script = dml(script).input(X=X, Y=Y, X_val=X_val, Y_val=Y_val).output(*outputs)\n",
    "W, b = ml.execute(script).get(*outputs)\n",
    "W, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script = \"\"\"\n",
    "source(\"breastcancer/softmax_clf.dml\") as clf\n",
    "\n",
    "# Hyperparameters & Settings\n",
    "lr = 5e-7  # learning rate\n",
    "mu = 0.5  # momentum\n",
    "decay = 0.999  # learning rate decay constant\n",
    "batch_size = 32\n",
    "epochs = 1\n",
    "log_interval = 10\n",
    "\n",
    "# Train\n",
    "[W, b] = clf::train(X, Y, X_val, Y_val, lr, mu, decay, batch_size, epochs, log_interval)\n",
    "\"\"\"\n",
    "outputs = (\"W\", \"b\")\n",
    "script = dml(script).input(X=X, Y=Y, X_val=X_val, Y_val=Y_val).output(*outputs)\n",
    "W, b = ml.execute(script).get(*outputs)\n",
    "W, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script = \"\"\"\n",
    "source(\"breastcancer/softmax_clf.dml\") as clf\n",
    "\n",
    "# Eval\n",
    "probs = clf::predict(X, W, b)\n",
    "[loss, accuracy] = clf::eval(probs, Y)\n",
    "probs_val = clf::predict(X_val, W, b)\n",
    "[loss_val, accuracy_val] = clf::eval(probs_val, Y_val)\n",
    "\"\"\"\n",
    "outputs = (\"loss\", \"accuracy\", \"loss_val\", \"accuracy_val\")\n",
    "script = dml(script).input(X=X, Y=Y, X_val=X_val, Y_val=Y_val, W=W, b=b).output(*outputs)\n",
    "loss, acc, loss_val, acc_val = ml.execute(script).get(*outputs)\n",
    "loss, acc, loss_val, acc_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# LeNet-like ConvNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity Check: Overfit Small Portion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script = \"\"\"\n",
    "source(\"breastcancer/convnet.dml\") as clf\n",
    "\n",
    "# Hyperparameters & Settings\n",
    "lr = 1e-2  # learning rate\n",
    "mu = 0.9  # momentum\n",
    "decay = 0.999  # learning rate decay constant\n",
    "lambda = 0  #5e-04\n",
    "batch_size = 32\n",
    "epochs = 300\n",
    "log_interval = 1\n",
    "dir = \"models/lenet-cnn/sanity/\"\n",
    "n = 200  # sample size for overfitting sanity check\n",
    "\n",
    "# Train\n",
    "[Wc1, bc1, Wc2, bc2, Wc3, bc3, Wa1, ba1, Wa2, ba2] = clf::train(X[1:n,], Y[1:n,], X[1:n,], Y[1:n,], C, Hin, Win, lr, mu, decay, lambda, batch_size, epochs, log_interval, dir)\n",
    "\"\"\"\n",
    "outputs = (\"Wc1\", \"bc1\", \"Wc2\", \"bc2\", \"Wc3\", \"bc3\", \"Wa1\", \"ba1\", \"Wa2\", \"ba2\")\n",
    "script = (dml(script).input(X=X, X_val=X_val, Y=Y, Y_val=Y_val,\n",
    "                            C=c, Hin=size, Win=size)\n",
    "                     .output(*outputs))\n",
    "Wc1, bc1, Wc2, bc2, Wc3, bc3, Wa1, ba1, Wa2, ba2 = ml.execute(script).get(*outputs)\n",
    "Wc1, bc1, Wc2, bc2, Wc3, bc3, Wa1, ba1, Wa2, ba2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script = \"\"\"\n",
    "source(\"breastcancer/convnet.dml\") as clf\n",
    "\n",
    "dir = \"models/lenet-cnn/hyperparam-search/\"\n",
    "\n",
    "# TODO: Fix `parfor` so that it can be efficiently used for hyperparameter tuning\n",
    "j = 1\n",
    "while(j < 2) {\n",
    "#parfor(j in 1:10000, par=6) {\n",
    "  # Hyperparameter Sampling & Settings\n",
    "  lr = 10 ^ as.scalar(rand(rows=1, cols=1, min=-7, max=-1))  # learning rate\n",
    "  mu = as.scalar(rand(rows=1, cols=1, min=0.5, max=0.9))  # momentum\n",
    "  decay = as.scalar(rand(rows=1, cols=1, min=0.9, max=1))  # learning rate decay constant\n",
    "  lambda = 10 ^ as.scalar(rand(rows=1, cols=1, min=-7, max=-1))  # regularization constant\n",
    "  batch_size = 32\n",
    "  epochs = 1\n",
    "  log_interval = 10\n",
    "  trial_dir = dir + \"j/\"\n",
    "\n",
    "  # Train\n",
    "  [Wc1, bc1, Wc2, bc2, Wc3, bc3, Wa1, ba1, Wa2, ba2] = clf::train(X, Y, X_val, Y_val, C, Hin, Win, lr, mu, decay, lambda, batch_size, epochs, log_interval, trial_dir)\n",
    "\n",
    "  # Eval\n",
    "  #probs = clf::predict(X, C, Hin, Win, Wc1, bc1, Wc2, bc2, Wc3, bc3, Wa1, ba1, Wa2, ba2)\n",
    "  #[loss, accuracy] = clf::eval(probs, Y)\n",
    "  probs_val = clf::predict(X_val, C, Hin, Win, Wc1, bc1, Wc2, bc2, Wc3, bc3, Wa1, ba1, Wa2, ba2)\n",
    "  [loss_val, accuracy_val] = clf::eval(probs_val, Y_val)\n",
    "\n",
    "  # Save hyperparams\n",
    "  str = \"lr: \" + lr + \", mu: \" + mu + \", decay: \" + decay + \", lambda: \" + lambda + \", batch_size: \" + batch_size\n",
    "  name = dir + accuracy_val + \",\" + j  #+\",\"+accuracy+\",\"+j\n",
    "  write(str, name)\n",
    "  j = j + 1\n",
    "}\n",
    "\"\"\"\n",
    "script = (dml(script).input(X=X, X_val=X_val, Y=Y, Y_val=Y_val, C=c, Hin=size, Win=size))\n",
    "ml.execute(script)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml.setStatistics(True)\n",
    "ml.setExplain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sc.setLogLevel(\"OFF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "script = \"\"\"\n",
    "source(\"breastcancer/convnet_distrib_sgd.dml\") as clf\n",
    "\n",
    "# Hyperparameters & Settings\n",
    "lr = 0.00205  # learning rate\n",
    "mu = 0.632  # momentum\n",
    "decay = 0.99  # learning rate decay constant\n",
    "lambda = 0.00385\n",
    "batch_size = 1\n",
    "parallel_batches = 19\n",
    "epochs = 1\n",
    "log_interval = 1\n",
    "dir = \"models/lenet-cnn/train/\"\n",
    "n = 50  #1216  # limit on number of samples (for debugging)\n",
    "X = X[1:n,]\n",
    "Y = Y[1:n,]\n",
    "X_val = X_val[1:n,]\n",
    "Y_val = Y_val[1:n,]\n",
    "\n",
    "# Train\n",
    "[Wc1, bc1, Wc2, bc2, Wc3, bc3, Wa1, ba1, Wa2, ba2] =\n",
    "    clf::train(X, Y, X_val, Y_val, C, Hin, Win, lr, mu, decay,\n",
    "               lambda, batch_size, parallel_batches, epochs,\n",
    "               log_interval, dir)\n",
    "\"\"\"\n",
    "outputs = (\"Wc1\", \"bc1\", \"Wc2\", \"bc2\", \"Wc3\", \"bc3\",\n",
    "           \"Wa1\", \"ba1\", \"Wa2\", \"ba2\")\n",
    "script = (dml(script).input(X=X, X_val=X_val, Y=Y, Y_val=Y_val,\n",
    "                            C=c, Hin=size, Win=size)\n",
    "                     .output(*outputs))\n",
    "outs = ml.execute(script).get(*outputs)\n",
    "Wc1, bc1, Wc2, bc2, Wc3, bc3, Wa1, ba1, Wa2, ba2 = outs\n",
    "Wc1, bc1, Wc2, bc2, Wc3, bc3, Wa1, ba1, Wa2, ba2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script = \"\"\"\n",
    "source(\"breastcancer/convnet_distrib_sgd.dml\") as clf\n",
    "\n",
    "# Hyperparameters & Settings\n",
    "lr = 0.00205  # learning rate\n",
    "mu = 0.632  # momentum\n",
    "decay = 0.99  # learning rate decay constant\n",
    "lambda = 0.00385\n",
    "batch_size = 1\n",
    "parallel_batches = 19\n",
    "epochs = 1\n",
    "log_interval = 1\n",
    "dir = \"models/lenet-cnn/train/\"\n",
    "\n",
    "# Dummy data\n",
    "[X, Y, C, Hin, Win] = clf::generate_dummy_data(50)  #1216)\n",
    "[X_val, Y_val, C, Hin, Win] = clf::generate_dummy_data(100)\n",
    "\n",
    "# Train\n",
    "[Wc1, bc1, Wc2, bc2, Wc3, bc3, Wa1, ba1, Wa2, ba2] =\n",
    "    clf::train(X, Y, X_val, Y_val, C, Hin, Win, lr, mu, decay,\n",
    "               lambda, batch_size, parallel_batches, epochs,\n",
    "               log_interval, dir)\n",
    "\"\"\"\n",
    "outputs = (\"Wc1\", \"bc1\", \"Wc2\", \"bc2\", \"Wc3\", \"bc3\",\n",
    "           \"Wa1\", \"ba1\", \"Wa2\", \"ba2\")\n",
    "script = dml(script).output(*outputs)\n",
    "outs = ml.execute(script).get(*outputs)\n",
    "Wc1, bc1, Wc2, bc2, Wc3, bc3, Wa1, ba1, Wa2, ba2 = outs\n",
    "Wc1, bc1, Wc2, bc2, Wc3, bc3, Wa1, ba1, Wa2, ba2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script = \"\"\"\n",
    "source(\"breastcancer/convnet_distrib_sgd.dml\") as clf\n",
    "\n",
    "# Eval\n",
    "probs = clf::predict(X, C, Hin, Win, Wc1, bc1, Wc2, bc2, Wc3, bc3, Wa1, ba1, Wa2, ba2)\n",
    "[loss, accuracy] = clf::eval(probs, Y)\n",
    "probs_val = clf::predict(X_val, C, Hin, Win, Wc1, bc1, Wc2, bc2, Wc3, bc3, Wa1, ba1, Wa2, ba2)\n",
    "[loss_val, accuracy_val] = clf::eval(probs_val, Y_val)\n",
    "\"\"\"\n",
    "outputs = (\"loss\", \"accuracy\", \"loss_val\", \"accuracy_val\")\n",
    "script = (dml(script).input(X=X, X_val=X_val, Y=Y, Y_val=Y_val,\n",
    "                            C=c, Hin=size, Win=size,\n",
    "                            Wc1=Wc1, bc1=bc1,\n",
    "                            Wc2=Wc2, bc2=bc2,\n",
    "                            Wc3=Wc3, bc3=bc3,\n",
    "                            Wa1=Wa1, ba1=ba1,\n",
    "                            Wa2=Wa2, ba2=ba2)\n",
    "                     .output(*outputs))\n",
    "loss, acc, loss_val, acc_val = ml.execute(script).get(*outputs)\n",
    "loss, acc, loss_val, acc_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script = \"\"\"\n",
    "source(\"breastcancer/convnet_distrib_sgd.dml\") as clf\n",
    "\n",
    "# Dummy data\n",
    "[X, Y, C, Hin, Win] = clf::generate_dummy_data(1216)\n",
    "[X_val, Y_val, C, Hin, Win] = clf::generate_dummy_data(100)\n",
    "\n",
    "# Eval\n",
    "probs = clf::predict(X, C, Hin, Win, Wc1, bc1, Wc2, bc2, Wc3, bc3, Wa1, ba1, Wa2, ba2)\n",
    "[loss, accuracy] = clf::eval(probs, Y)\n",
    "probs_val = clf::predict(X_val, C, Hin, Win, Wc1, bc1, Wc2, bc2, Wc3, bc3, Wa1, ba1, Wa2, ba2)\n",
    "[loss_val, accuracy_val] = clf::eval(probs_val, Y_val)\n",
    "\"\"\"\n",
    "outputs = (\"loss\", \"accuracy\", \"loss_val\", \"accuracy_val\")\n",
    "script = (dml(script).input(Wc1=Wc1, bc1=bc1,\n",
    "                            Wc2=Wc2, bc2=bc2,\n",
    "                            Wc3=Wc3, bc3=bc3,\n",
    "                            Wa1=Wa1, ba1=ba1,\n",
    "                            Wa2=Wa2, ba2=ba2)\n",
    "                     .output(*outputs))\n",
    "loss, acc, loss_val, acc_val = ml.execute(script).get(*outputs)\n",
    "loss, acc, loss_val, acc_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# script = \"\"\"\n",
    "# N = 102400  # num examples\n",
    "# C = 3  # num input channels\n",
    "# Hin = 256  # input height\n",
    "# Win = 256  # input width\n",
    "# X = rand(rows=N, cols=C*Hin*Win, pdf=\"normal\")\n",
    "# \"\"\"\n",
    "# outputs = \"X\"\n",
    "# script = dml(script).output(*outputs)\n",
    "# thisX = ml.execute(script).get(*outputs)\n",
    "# thisX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# script = \"\"\"\n",
    "# f = function(matrix[double] X) return(matrix[double] Y) {\n",
    "#   if (1==1) {}\n",
    "#   a = as.scalar(rand(rows=1, cols=1))\n",
    "#   Y = X * a\n",
    "# }\n",
    "# Y = f(X)\n",
    "# \"\"\"\n",
    "# outputs = \"Y\"\n",
    "# script = dml(script).input(X=thisX).output(*outputs)\n",
    "# thisY = ml.execute(script).get(*outputs)\n",
    "# thisY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 + Spark 2.x + SystemML",
   "language": "python",
   "name": "pyspark3_2.x"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
