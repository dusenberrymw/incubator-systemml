{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Read in train & val data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_df = sqlContext.read.load(\"data/train_100_grayscale.parquet\")\n",
    "val_df = sqlContext.read.load(\"data/val_100_grayscale.parquet\")\n",
    "train_df, val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tc = train_df.count()\n",
    "vc = val_df.count()\n",
    "tc, vc  # 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_df.select(\"tumor_score\").groupBy(\"tumor_score\").count().show()\n",
    "val_df.select(\"tumor_score\").groupBy(\"tumor_score\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "C = 1\n",
    "SIZE = train_df.first().sample.toArray().shape[0]\n",
    "SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def extract_data(df):\n",
    "  rows = df.collect()\n",
    "  x = np.array([row.sample.toArray().astype(np.float32) for row in rows])\n",
    "  y = np.array([row.tumor_score for row in rows])\n",
    "  return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Convert data to TFRecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "\n",
    "def _bytes_feature(value):\n",
    "  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _float_feature(value):\n",
    "  return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "def _int64_feature(value):\n",
    "  return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "def encode_and_write(rows, folder):\n",
    "  \"\"\"Convert data to file of tfrecords.\"\"\"\n",
    "  # TODO: A uuid is highly unlikely to overlap, but we implement a truly\n",
    "  # unique identifier.  We could possibly `mapPartitions` to lists, then\n",
    "  # `zipWithUniqueID` on each list, then write each entry, using the ID\n",
    "  # as the filename.\n",
    "  filename = os.path.join(folder, str(uuid.uuid4()) + \".tfrecords\")\n",
    "  writer = tf.python_io.TFRecordWriter(filename)\n",
    "  for row in rows:\n",
    "    example = tf.train.Example(features=tf.train.Features(feature={\n",
    "          '__INDEX': _int64_feature(row['__INDEX']),\n",
    "          'slide_num': _int64_feature(row.slide_num),\n",
    "          'tumor_score': _int64_feature(row.tumor_score),\n",
    "          'molecular_score': _float_feature(row.molecular_score),\n",
    "          'sample': _bytes_feature(row.sample.toArray().astype(np.float32).tostring()) # should use the uint8 DataFrame\n",
    "          # TODO: ENCODE SHAPE\n",
    "        }))\n",
    "    writer.write(example.SerializeToString())\n",
    "  writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def read_and_decode(filename_queue):\n",
    "  \"\"\"Read TFRecords from a file and decode into single examples.\"\"\"\n",
    "  reader = tf.TFRecordReader()\n",
    "  filename, serialized_example = reader.read(filename_queue)\n",
    "  features = tf.parse_single_example(\n",
    "      serialized_example,\n",
    "      features={\n",
    "          '__INDEX': tf.FixedLenFeature([], tf.int64),\n",
    "          'slide_num': tf.FixedLenFeature([], tf.int64),\n",
    "          'tumor_score': tf.FixedLenFeature([], tf.int64),\n",
    "          'molecular_score': tf.FixedLenFeature([], tf.float32), # default for Python float is float32\n",
    "          'sample': tf.FixedLenFeature([], tf.string)\n",
    "          # TODO: DECODE SHAPE\n",
    "      })\n",
    "  index = features[\"__INDEX\"]  #tf.cast(features[\"__INDEX\"], tf.int8)\n",
    "  slide_num = features[\"slide_num\"]  #tf.cast(features[\"slide_num\"], tf.int32)\n",
    "  tumor_score = features[\"tumor_score\"]\n",
    "  molecular_score = features[\"molecular_score\"]\n",
    "  sample = tf.decode_raw(features[\"sample\"], tf.float32)  # should use the uint8 DataFrame\n",
    "  \n",
    "  return index, slide_num, tumor_score, molecular_score, sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Convert rdd partitions -> tfrecord files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "base_dir = \"data/tf\"\n",
    "train_dir = os.path.join(base_dir, \"train\")\n",
    "val_dir = os.path.join(base_dir, \"val\")\n",
    "for folder in [train_dir, val_dir]:\n",
    "  if not os.path.exists(folder):\n",
    "    os.mkdir(folder)\n",
    "train_dir, val_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# TODO: Try `mapPartitionsWithIndex` and forced file writing side-effect by calling `collect()`.\n",
    "train_df.repartition(3).foreachPartition(lambda rows: encode_and_write(rows, train_dir))\n",
    "val_df.repartition(3).foreachPartition(lambda rows: encode_and_write(rows, val_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Create convnet model graph\n",
    "Create network:\n",
    "  conv1 -> relu1 -> pool1 -> conv2 -> relu2 -> pool2 -> conv3 -> relu3 -> pool3 -> affine1 -> relu1 -> affine2 -> softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Hyperparams & Settings\n",
    "folder=train_dir\n",
    "min_after_dequeue = 1000\n",
    "batch_size = 100  #64\n",
    "capacity = min_after_dequeue + 4 * batch_size\n",
    "num_threads = 2\n",
    "num_epochs = 100\n",
    "classes = 3\n",
    "features = 65536\n",
    "C = 1  # Number of input channels (dimensionality of input depth)\n",
    "Hin = 256  # Input height\n",
    "Win = 256  # Input width\n",
    "Hf = 3  # conv filter height\n",
    "Wf = 3  # conv filter width\n",
    "Hfp = 2  # pool filter height\n",
    "Wfp = 2  # pool filter width\n",
    "stride = 1  # conv stride\n",
    "pstride = 2  # pool stride\n",
    "pad = 1  # For same dimensions, (Hf - stride) / 2\n",
    "F1 = 32  # num conv filters in conv1\n",
    "F2 = 32  # num conv filters in conv2\n",
    "F3 = 32  # num conv filters in conv3\n",
    "N1 = 512  # num nodes in affine1\n",
    "lr = 1e-4 # learning rate\n",
    "\n",
    "# Inputs\n",
    "with tf.name_scope(\"input\") as scope:\n",
    "  # Input queues\n",
    "  filenames = [os.path.join(folder, f) for f in os.listdir(folder)]\n",
    "  filename_queue = tf.train.string_input_producer(filenames, num_epochs=num_epochs)\n",
    "  index, slide_num, tumor_score, molecular_score, sample = read_and_decode(filename_queue)\n",
    "  sample.set_shape([Hin*Win])\n",
    "  x, y_ = tf.train.shuffle_batch([sample, tumor_score], batch_size=batch_size, capacity=capacity,\n",
    "                                 min_after_dequeue=min_after_dequeue, num_threads=num_threads)\n",
    "  \n",
    "  #x = tf.placeholder(tf.float32, [None, features], name=\"x\")\n",
    "  x_image = tf.transpose(tf.reshape(x, [-1, C, Hin, Win]), perm=[0,2,3,1])  # shape (N,H,W,C)\n",
    "  #y_ = tf.placeholder(tf.int64, [None, ], name=\"y_\")\n",
    "  y_one_hot = tf.one_hot(y_-1, classes)  # or use sparse cross entropy\n",
    "  tf.summary.image(\"x\", x_image)\n",
    "  tf.summary.histogram(\"y\", y_)\n",
    "\n",
    "# Conv layer 1: conv1 -> relu1 -> pool1\n",
    "with tf.name_scope(\"conv1\") as scope:\n",
    "  W = tf.Variable(tf.random_normal([Hf, Wf, C, F1]) * np.sqrt(2.0/(Hf*Wf*C)), name=\"W\")\n",
    "  b = tf.Variable(tf.zeros([F1]), name=\"b\")\n",
    "  conv = tf.nn.conv2d(x_image, W, [1,stride,stride,1], padding=\"SAME\") + b\n",
    "  relu = tf.nn.relu(conv)\n",
    "  pool = tf.nn.max_pool(relu, ksize=[1,Hfp,Wfp,1], strides=[1,pstride,pstride,1], padding=\"SAME\")\n",
    "  tf.summary.image(\"conv1\", tf.transpose(W, [3,0,1,2]), max_outputs=F1)  # transpose to [F1,H,W,C]\n",
    "\n",
    "# Conv layer 2: conv2 -> relu2 -> pool2\n",
    "with tf.name_scope(\"conv2\") as scope:\n",
    "  W = tf.Variable(tf.random_normal([Hf, Wf, F1, F2]) * np.sqrt(2.0/(Hf*Wf*F1)), name=\"W\")\n",
    "  b = tf.Variable(tf.zeros([F2]), name=\"b\")\n",
    "  conv = tf.nn.conv2d(pool, W, [1,stride,stride,1], padding=\"SAME\") + b\n",
    "  relu = tf.nn.relu(conv)\n",
    "  pool = tf.nn.max_pool(relu, ksize=[1,Hfp,Wfp,1], strides=[1,pstride,pstride,1], padding=\"SAME\")\n",
    "\n",
    "# Conv layer 3: conv3 -> relu3 -> pool3\n",
    "with tf.name_scope(\"conv3\") as scope:\n",
    "  W = tf.Variable(tf.random_normal([Hf, Wf, F2, F3]) * np.sqrt(2.0/(Hf*Wf*F2)), name=\"W\")\n",
    "  b = tf.Variable(tf.zeros([F3]), name=\"b\")\n",
    "  conv = tf.nn.conv2d(pool, W, [1,stride,stride,1], padding=\"SAME\") + b\n",
    "  relu = tf.nn.relu(conv)\n",
    "  pool = tf.nn.max_pool(relu, ksize=[1,Hfp,Wfp,1], strides=[1,pstride,pstride,1], padding=\"SAME\")\n",
    "\n",
    "# Affine layer 1:  affine1 -> relu1 -> dropout\n",
    "with tf.name_scope(\"affine1\") as scope:\n",
    "  D = int(F3*(Hin/2**3)*(Win/2**3))\n",
    "  W = tf.Variable(tf.random_normal([D,N1]) * np.sqrt(2.0/D), name=\"W\")\n",
    "  b = tf.Variable(tf.zeros([N1]), name=\"b\")\n",
    "  affine = tf.matmul(tf.reshape(pool, [-1,D]), W) + b\n",
    "  relu = tf.nn.relu(affine)\n",
    "  keep_prob = tf.placeholder(tf.float32, name=\"keep_prob\")\n",
    "  dropout = tf.nn.dropout(relu, keep_prob)\n",
    "\n",
    "# Affine layer 2:  affine2 -> softmax\n",
    "with tf.name_scope(\"affine2\") as scope:\n",
    "  W = tf.Variable(tf.random_normal([N1,classes]) * np.sqrt(2.0/N1), name=\"W\")\n",
    "  b = tf.Variable(tf.zeros([classes]), name=\"b\")\n",
    "  logits = tf.matmul(dropout, W) + b\n",
    "  probs = tf.nn.softmax(logits)\n",
    "  tf.summary.histogram(\"logits\", logits)\n",
    "  tf.summary.histogram(\"probs\", probs)\n",
    "\n",
    "# Loss\n",
    "with tf.name_scope(\"loss\") as scope:\n",
    "  cross_entropy_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, y_one_hot))\n",
    "  tf.summary.scalar(\"loss\", cross_entropy_loss)\n",
    "\n",
    "# Train\n",
    "# train_step = tf.train.GradientDescentOptimizer(lr).minimize(cross_entropy)\n",
    "train_step = tf.train.AdamOptimizer(lr).minimize(cross_entropy_loss)\n",
    "\n",
    "# Eval metrics\n",
    "with tf.name_scope(\"eval\") as scope:\n",
    "  correct_pred = tf.equal(tf.argmax(logits,1), tf.argmax(y_one_hot,1))\n",
    "  accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "  tf.summary.scalar(\"accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Run `tensorboard --logdir=tf_logs --host=localhost --debug --reload_interval 5`\n",
    "with tf.Session() as sess:\n",
    "  # Summaries\n",
    "  log_dir = \"tf_logs\"\n",
    "  summary_op = tf.summary.merge_all()\n",
    "  train_writer = tf.train.SummaryWriter(log_dir + \"/train\", sess.graph)\n",
    "  val_writer = tf.train.SummaryWriter(log_dir + \"/val\")\n",
    "  \n",
    "  init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "  sess.run(init_op)\n",
    "  \n",
    "  # Start input queues\n",
    "  coord = tf.train.Coordinator()\n",
    "  threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "  \n",
    "  # Data Gen\n",
    "  #train_generator = gen_batch(train_df.rdd, 64)\n",
    "  x_val, y_val = extract_data(val_df)\n",
    "  \n",
    "  # Train\n",
    "  try:\n",
    "    i = 0\n",
    "    while not coord.should_stop():\n",
    "      _ = sess.run([train_step], feed_dict={keep_prob:0.5})\n",
    "      if i % 10 == 0:\n",
    "        # train stats\n",
    "        summary, train_acc = sess.run([summary_op, accuracy], feed_dict={keep_prob:0.5})\n",
    "        train_writer.add_summary(summary, i)\n",
    "        # val stats\n",
    "        summary, val_acc = sess.run([summary_op, accuracy], feed_dict={x: x_val, y_:y_val, keep_prob:1})\n",
    "        val_writer.add_summary(summary, i)\n",
    "        #train_writer.flush()  # To force write -- probably slower (usually asynchronous)\n",
    "        #val_writer.flush()  # To force write -- probably slower (usually asynchronous)\n",
    "        print(\"Iter: {}, \\t Train Accuracy: {:.4f}, \\t Val Accuracy: {:.4f}\".format(i, train_acc, val_acc))\n",
    "      i += 1\n",
    "  except tf.errors.OutOfRangeError:\n",
    "    print(\"Done training!\")\n",
    "  finally:\n",
    "    # Ask threads to stop when finished\n",
    "    coord.request_stop()\n",
    "  # Wait for all threads to finish  \n",
    "  coord.join(threads)\n",
    "  \n",
    "train_writer.flush()  # Make sure everything is written before exiting\n",
    "val_writer.flush()  # Make sure everything is written before exiting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 + Spark 1.6 + SystemML",
   "language": "python",
   "name": "pyspark3_1.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
