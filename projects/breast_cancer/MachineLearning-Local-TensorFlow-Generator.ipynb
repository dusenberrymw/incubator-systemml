{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Read in train & val data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_df = sqlContext.read.load(\"data/train_100_grayscale.parquet\")\n",
    "val_df = sqlContext.read.load(\"data/val_100_grayscale.parquet\")\n",
    "train_df, val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tc = train_df.count()\n",
    "vc = val_df.count()\n",
    "tc, vc  # 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_df.select(\"tumor_score\").groupBy(\"tumor_score\").count().show()\n",
    "val_df.select(\"tumor_score\").groupBy(\"tumor_score\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "C = 1\n",
    "SIZE = train_df.first().sample.toArray().shape[0]\n",
    "SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Create batch generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def gen_batch(rdd, batch_size=32):\n",
    "  \"\"\"\n",
    "  RDD data generator.\n",
    "  \n",
    "  Generator that cycles through the data and yields a\n",
    "  batch at a time, reinitializing the iterator as needed\n",
    "  to continue yielding batches.\n",
    "  \n",
    "  Args:\n",
    "    rdd: A PySpark RDD containing the training data.\n",
    "    batch_size: Size of batches to return.\n",
    "  \"\"\"\n",
    "  iterator = rdd.toLocalIterator()\n",
    "  while True:\n",
    "    features = []\n",
    "    labels = []\n",
    "    for i in range(batch_size):\n",
    "      # Generate batch\n",
    "      try:\n",
    "        row = next(iterator)\n",
    "      except StopIteration:\n",
    "        # Restart iterator\n",
    "        iterator = rdd.toLocalIterator()\n",
    "        row = next(iterator)\n",
    "      features.append(row.sample.values)\n",
    "      labels.append(row.tumor_score)\n",
    "    x_batch = np.array(features).astype(np.float32)\n",
    "    y_batch = np.array(labels).astype(np.int32)\n",
    "    yield x_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "generator = gen_batch(train_df.rdd, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "x, y = next(generator)\n",
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "x.shape, x.dtype, y.shape, y.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Get validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "rows = val_df.take(3)  #.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "x = np.array([row.sample.toArray().astype(np.float32) for row in rows])\n",
    "y = np.array([row.tumor_score for row in rows])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "x.dtype, x.shape, y.dtype, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def extract_data(df):\n",
    "  \"\"\"\n",
    "  Extract data from a PySpark DataFrame into a NumPy array.\n",
    "  \"\"\"\n",
    "  rows = df.collect()\n",
    "  x = np.array([row.sample.toArray().astype(np.float32) for row in rows])\n",
    "  y = np.array([row.tumor_score for row in rows])\n",
    "  return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Create convnet model graph\n",
    "Create network:\n",
    "  conv1 -> relu1 -> pool1 -> conv2 -> relu2 -> pool2 -> conv3 -> relu3 -> pool3 -> affine1 -> relu1 -> affine2 -> softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Hyperparams & Settings\n",
    "classes = 3\n",
    "features = 65536\n",
    "C = 1  # Number of input channels (dimensionality of input depth)\n",
    "Hin = 256  # Input height\n",
    "Win = 256  # Input width\n",
    "Hf = 3  # conv filter height\n",
    "Wf = 3  # conv filter width\n",
    "Hfp = 2  # pool filter height\n",
    "Wfp = 2  # pool filter width\n",
    "stride = 1  # conv stride\n",
    "pstride = 2  # pool stride\n",
    "pad = 1  # For same dimensions, (Hf - stride) / 2\n",
    "F1 = 32  # num conv filters in conv1\n",
    "F2 = 32  # num conv filters in conv2\n",
    "F3 = 32  # num conv filters in conv3\n",
    "N1 = 512  # num nodes in affine1\n",
    "lr = 1e-4 # learning rate\n",
    "\n",
    "# Inputs\n",
    "with tf.name_scope(\"input\") as scope:\n",
    "  x = tf.placeholder(tf.float32, [None, features], name=\"x\")\n",
    "  x_image = tf.transpose(tf.reshape(x, [-1, C, Hin, Win]), perm=[0,2,3,1])  # shape (N,H,W,C)\n",
    "  y_ = tf.placeholder(tf.int64, [None, ], name=\"y_\")\n",
    "  y_one_hot = tf.one_hot(y_-1, classes)  # or use sparse cross entropy\n",
    "  tf.summary.image(\"x\", x_image)\n",
    "  tf.summary.histogram(\"y\", y_)\n",
    "\n",
    "# Conv layer 1: conv1 -> relu1 -> pool1\n",
    "with tf.name_scope(\"conv1\") as scope:\n",
    "  W = tf.Variable(tf.random_normal([Hf, Wf, C, F1]) * np.sqrt(2.0/(Hf*Wf*C)), name=\"W\")\n",
    "  b = tf.Variable(tf.zeros([F1]), name=\"b\")\n",
    "  conv = tf.nn.conv2d(x_image, W, [1,stride,stride,1], padding=\"SAME\") + b\n",
    "  relu = tf.nn.relu(conv)\n",
    "  pool = tf.nn.max_pool(relu, ksize=[1,Hfp,Wfp,1], strides=[1,pstride,pstride,1], padding=\"SAME\")\n",
    "  tf.summary.image(\"conv1\", tf.transpose(W, [3,0,1,2]), max_outputs=F1)  # transpose to [N,H,W,C]\n",
    "\n",
    "# Conv layer 2: conv2 -> relu2 -> pool2\n",
    "with tf.name_scope(\"conv2\") as scope:\n",
    "  W = tf.Variable(tf.random_normal([Hf, Wf, F1, F2]) * np.sqrt(2.0/(Hf*Wf*F1)), name=\"W\")\n",
    "  b = tf.Variable(tf.zeros([F2]), name=\"b\")\n",
    "  conv = tf.nn.conv2d(pool, W, [1,stride,stride,1], padding=\"SAME\") + b\n",
    "  relu = tf.nn.relu(conv)\n",
    "  pool = tf.nn.max_pool(relu, ksize=[1,Hfp,Wfp,1], strides=[1,pstride,pstride,1], padding=\"SAME\")\n",
    "\n",
    "# Conv layer 3: conv3 -> relu3 -> pool3\n",
    "with tf.name_scope(\"conv3\") as scope:\n",
    "  W = tf.Variable(tf.random_normal([Hf, Wf, F2, F3]) * np.sqrt(2.0/(Hf*Wf*F2)), name=\"W\")\n",
    "  b = tf.Variable(tf.zeros([F3]), name=\"b\")\n",
    "  conv = tf.nn.conv2d(pool, W, [1,stride,stride,1], padding=\"SAME\") + b\n",
    "  relu = tf.nn.relu(conv)\n",
    "  pool = tf.nn.max_pool(relu, ksize=[1,Hfp,Wfp,1], strides=[1,pstride,pstride,1], padding=\"SAME\")\n",
    "\n",
    "# Affine layer 1:  affine1 -> relu1 -> dropout\n",
    "with tf.name_scope(\"affine1\") as scope:\n",
    "  D = int(F3*(Hin/2**3)*(Win/2**3))\n",
    "  W = tf.Variable(tf.random_normal([D,N1]) * np.sqrt(2.0/D), name=\"W\")\n",
    "  b = tf.Variable(tf.zeros([N1]), name=\"b\")\n",
    "  affine = tf.matmul(tf.reshape(pool, [-1,D]), W) + b\n",
    "  relu = tf.nn.relu(affine)\n",
    "  keep_prob = tf.placeholder(tf.float32, name=\"keep_prob\")\n",
    "  dropout = tf.nn.dropout(relu, keep_prob)\n",
    "\n",
    "# Affine layer 2:  affine2 -> softmax\n",
    "with tf.name_scope(\"affine2\") as scope:\n",
    "  W = tf.Variable(tf.random_normal([N1,classes]) * np.sqrt(2.0/N1), name=\"W\")\n",
    "  b = tf.Variable(tf.zeros([classes]), name=\"b\")\n",
    "  logits = tf.matmul(dropout, W) + b\n",
    "  probs = tf.nn.softmax(logits)\n",
    "  tf.summary.histogram(\"logits\", logits)\n",
    "  tf.summary.histogram(\"probs\", probs)\n",
    "\n",
    "# Loss\n",
    "with tf.name_scope(\"loss\") as scope:\n",
    "  cross_entropy_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, y_one_hot))\n",
    "  tf.summary.scalar(\"loss\", cross_entropy_loss)\n",
    "\n",
    "# Train\n",
    "# train_step = tf.train.GradientDescentOptimizer(lr).minimize(cross_entropy)\n",
    "train_step = tf.train.AdamOptimizer(lr).minimize(cross_entropy_loss)\n",
    "\n",
    "# Eval metrics\n",
    "with tf.name_scope(\"eval\") as scope:\n",
    "  correct_pred = tf.equal(tf.argmax(logits,1), tf.argmax(y_one_hot,1))\n",
    "  accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "  tf.summary.scalar(\"accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Run `tensorboard --logdir=tf_logs --host=localhost --debug --reload_interval 5`\n",
    "with tf.Session() as sess:\n",
    "  # Summaries\n",
    "  log_dir = \"tf_logs\"\n",
    "  summary_op = tf.summary.merge_all()\n",
    "  train_writer = tf.train.SummaryWriter(log_dir + \"/train\", sess.graph)\n",
    "  val_writer = tf.train.SummaryWriter(log_dir + \"/val\")\n",
    "  \n",
    "  # Data Gen\n",
    "  train_generator = gen_batch(train_df.rdd, 64)\n",
    "  x_val, y_val = extract_data(val_df)\n",
    "  \n",
    "  # Train\n",
    "  sess.run(tf.global_variables_initializer())\n",
    "  steps = 100\n",
    "  for i in range(steps):\n",
    "    xs, ys = next(train_generator)\n",
    "    _ = sess.run([train_step], feed_dict={x: xs, y_:ys, keep_prob:0.5})\n",
    "    if i % 10 == 0:\n",
    "      # train stats\n",
    "      summary, train_acc = sess.run([summary_op, accuracy], feed_dict={x: xs, y_:ys, keep_prob:0.5})\n",
    "      train_writer.add_summary(summary, i)\n",
    "      # val stats\n",
    "      summary, val_acc = sess.run([summary_op, accuracy], feed_dict={x: x_val, y_:y_val, keep_prob:1})\n",
    "      val_writer.add_summary(summary, i)\n",
    "      print(\"Iter: {}, \\t Train Accuracy: {:.4f}, \\t Val Accuracy: {:.4f}\".format(i, train_acc, val_acc))\n",
    "  train_writer.flush()  # Make sure everything is written before exiting\n",
    "  val_writer.flush()  # Make sure everything is written before exiting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# 1. Add TensorBoard summaries and track.\n",
    "# 2. Plug into larger dataset.\n",
    "# 3. Run on cluster.\n",
    "# 4. Explore saving to TFRecord format, then reading from files shared on DFS (gfs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 + Spark 1.6 + SystemML",
   "language": "python",
   "name": "pyspark3_1.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
