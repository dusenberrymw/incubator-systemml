{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import math\n",
    "import multiprocessing as mp\n",
    "import queue\n",
    "import shutil\n",
    "import threading\n",
    "import time\n",
    "\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in train & val data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train_df = sqlContext.read.load(\"data/train_0.01_sample_grayscale_64.parquet\")\n",
    "# val_df = sqlContext.read.load(\"data/val_0.01_sample_grayscale_64.parquet\")\n",
    "\n",
    "# train_df = sqlContext.read.load(\"data/train_0.01_sample_64.parquet\")\n",
    "# val_df = sqlContext.read.load(\"data/val_0.01_sample_64.parquet\")\n",
    "\n",
    "train_df = sqlContext.read.load(\"data/train.parquet\")\n",
    "val_df = sqlContext.read.load(\"data/val.parquet\")\n",
    "train_df, val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tc = train_df.count()\n",
    "vc = val_df.count()\n",
    "tc, vc"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "for df in [train_df, val_df, train_df_sample, val_df_sample]:\n",
    "  df.select(\"tumor_score\").groupBy(\"tumor_score\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "p=0.001  #0.02  #0.2\n",
    "train_df_sample = train_df.sampleBy(\"tumor_score\", fractions={1: p, 2: p, 3: p}, seed=42)\n",
    "val_df_sample = val_df.sampleBy(\"tumor_score\", fractions={1: p, 2: p, 3: p}, seed=42)\n",
    "train_df_sample.cache(), val_df_sample.cache()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tsc = train_df_sample.count()\n",
    "vsc = val_df_sample.count()\n",
    "tsc, vsc"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "for df in [train_df_sample, val_df_sample]:\n",
    "  df.select(\"tumor_score\").groupBy(\"tumor_score\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## TODO: Switch to Spark 2.0 once [SPARK-18281](https://issues.apache.org/jira/browse/SPARK-18281) is fixed to take advantage of faster `toLocalIterator()` on `DataFrames`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CHANNELS = 3\n",
    "FEATURES = train_df.first().sample.toArray().shape[0]\n",
    "SIZE = int(math.sqrt(FEATURES/CHANNELS))\n",
    "CLASSES = 3\n",
    "FEATURES, SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create batch generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_batch(rdd, batch_size=32):\n",
    "  \"\"\"\n",
    "  RDD data generator.\n",
    "  \n",
    "  Generator that cycles through the data and yields a\n",
    "  batch at a time, reinitializing the iterator as needed\n",
    "  to continue yielding batches.\n",
    "  \n",
    "  Args:\n",
    "    rdd: A PySpark RDD containing the training data.\n",
    "    batch_size: Size of batches to return.\n",
    "  \"\"\"\n",
    "  rdd.cache()\n",
    "  iterator = rdd.toLocalIterator()\n",
    "  while True:\n",
    "    features = []\n",
    "    labels = []\n",
    "    for i in range(batch_size):\n",
    "      # Generate batch\n",
    "      try:\n",
    "        row = next(iterator)\n",
    "      except StopIteration:\n",
    "        # Restart iterator\n",
    "        iterator = rdd.toLocalIterator()\n",
    "        row = next(iterator)\n",
    "      features.append(row.sample.values)\n",
    "      labels.append(row.tumor_score)\n",
    "    x_batch = np.array(features).astype(np.uint8)\n",
    "    y_batch = np.array(labels).astype(np.uint8)\n",
    "    yield x_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class QueueingGenerator(threading.Thread):\n",
    "  \"\"\"Class that transforms a generator into a asynchronous, background queuing generator.\n",
    "\n",
    "  See http://stackoverflow.com/questions/7323664/python-generator-pre-fetch\n",
    "  and https://github.com/justheuristic/prefetch_generator.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, generator, capacity):\n",
    "    \"\"\"Transform a generator into a background generator.\n",
    "\n",
    "    Args:\n",
    "      generator: A generator function.\n",
    "      capacity: Size of the queue.\n",
    "\n",
    "      Returns:\n",
    "        A new generator that fetches results from the given generator\n",
    "        in a separate thread and fills a buffer queue. \n",
    "    \"\"\"\n",
    "    threading.Thread.__init__(self)\n",
    "    self.queue = queue.Queue(capacity)\n",
    "    self.generator = generator\n",
    "    self.daemon = True\n",
    "    self.start()\n",
    "    self._stop = threading.Event()\n",
    "\n",
    "  def run(self):\n",
    "    for item in self.generator:\n",
    "      if self._stop.is_set():\n",
    "        break\n",
    "      self.queue.put(item)\n",
    "    self.queue.put(None)\n",
    "\n",
    "  def __next__(self):\n",
    "    item = self.queue.get()\n",
    "    if item is None:\n",
    "      raise StopIteration\n",
    "    return item\n",
    "  \n",
    "  def stop(self):\n",
    "    self._stop.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fill_partition_num_queue(partition_num_queue, rdd):\n",
    "  partitions = rdd.getNumPartitions()\n",
    "  while True:\n",
    "    for i in range(partitions):\n",
    "      partition_num_queue.put(i)\n",
    "\n",
    "def fill_partition_queue(partition_queue, partition_num_queue, rdd):\n",
    "  while True:\n",
    "    partition = partition_num_queue.get()\n",
    "    rows = rdd.context.runJob(rdd, lambda x: x, [partition])\n",
    "    partition_queue.put(rows)\n",
    "\n",
    "def fill_row_queue(row_queue, partition_queue):\n",
    "  while True:\n",
    "    rows = partition_queue.get()\n",
    "    for row in rows:\n",
    "      row_queue.put(row)\n",
    "\n",
    "# TODO: Maybe use this for TensorFlow feeding directly.\n",
    "# def fill_batch_queue(batch_queue, row_queue, batch_size=32):\n",
    "#   while True:\n",
    "#     features = []\n",
    "#     labels = []\n",
    "#     for i in range(batch_size):\n",
    "#       # Generate batch\n",
    "#       row = row_queue.get()\n",
    "#       features.append(row.sample.values)\n",
    "#       labels.append(row.tumor_score)\n",
    "#     x_batch = np.array(features).astype(np.uint8)\n",
    "#     y_batch = np.array(labels).astype(np.uint8)\n",
    "#     batch_queue.put((x_batch, y_batch))\n",
    "\n",
    "# def gen_batch1(batch_queue):\n",
    "#   while True:\n",
    "#     x_batch, y_batch = batch_queue.get()\n",
    "#     yield x_batch, y_batch\n",
    "    \n",
    "def gen_batch2(row_queue, batch_size=32):\n",
    "  while True:\n",
    "    features = []\n",
    "    labels = []\n",
    "    for i in range(batch_size):\n",
    "      # Generate batch\n",
    "      row = row_queue.get()\n",
    "      features.append(row.sample.values)\n",
    "      labels.append(row.tumor_score)\n",
    "    x_batch = np.array(features).astype(np.uint8)\n",
    "    y_batch = np.array(labels).astype(np.uint8)\n",
    "    yield x_batch, y_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: Clean this up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "partition_num_queue = mp.Queue(1000)\n",
    "partition_queue = mp.Queue(100)\n",
    "row_queue = mp.Queue(1000)\n",
    "# batch_queue = mp.Queue(1000)\n",
    "\n",
    "num_partition_threads = 20\n",
    "num_row_processes = 4\n",
    "# num_batch_processes = 2\n",
    "\n",
    "rdd = train_df.rdd\n",
    "\n",
    "partition_num_process = mp.Process(target=fill_partition_num_queue, args=(partition_num_queue, rdd), daemon=True)\n",
    "# partition_processes = [mp.Process(target=fill_partition_queue, args=(partition_queue, partition_num_queue, rdd), daemon=True) for _ in range(num_partition_processes)]\n",
    "partition_threads = [threading.Thread(target=fill_partition_queue, args=(partition_queue, partition_num_queue, rdd), daemon=True) for _ in range(num_partition_threads)]\n",
    "row_processes = [mp.Process(target=fill_row_queue, args=(row_queue, partition_queue), daemon=True) for _ in range(num_row_processes)]\n",
    "# batch_processes = [mp.Process(target=fill_batch_queue, args=(batch_queue, row_queue), daemon=True) for _ in range(num_batch_processes)]\n",
    "\n",
    "ps = [partition_num_process] + row_processes #+ batch_processes\n",
    "\n",
    "for p in partition_threads + ps:\n",
    "  p.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "partition_num_queue.qsize(), partition_queue.qsize(), row_queue.qsize()#, batch_queue.qsize()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for p in ps:\n",
    "  p.terminate()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "a = gen_batch(batch_queue)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "next(a)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "min_after_dequeue = 1000\n",
    "batch_size = 32  #64\n",
    "batches = math.ceil(10000/batch_size)\n",
    "capacity = min_after_dequeue + 4 * batch_size\n",
    "capacity"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "generator = gen_batch(train_df.rdd, batch_size)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "x, y = next(generator)\n",
    "x, y"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "x.shape, x.dtype, y.shape, y.dtype"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "bg_gen = QueueingGenerator(gen_batch(train_df.rdd, batch_size), capacity)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "bg_gen.queue.qsize()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "x, y = next(bg_gen)\n",
    "x, y"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "bg_gen.queue.qsize()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# TODO: Test speed of this, and adjust capacity until it is saturated --> Answer: ~1000\n",
    "# TODO: Figure out how to stop thread --> Answer: will stop when Python program exits"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "bg_gen.stop()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "bg_gen.queue.qsize()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "math.ceil(10000/batch_size)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "%%time\n",
    "gen = gen_batch(train_df.rdd, batch_size)\n",
    "for i in range(batches):\n",
    "  x, y = next(gen)\n",
    "  time.sleep(0.1)\n",
    "  print(\"-\", end=\" \")\n",
    "print()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "%%time\n",
    "gen = gen_batch(train_df.rdd, batch_size)\n",
    "bg_gen = QueueingGenerator(gen, capacity)\n",
    "for i in range(batches):\n",
    "  x, y = next(bg_gen)\n",
    "  time.sleep(0.1)\n",
    "  print(\"-\", end=\" \")\n",
    "print()\n",
    "bg_gen.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rows = val_df.take(3)  #.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = np.array([row.sample.toArray().astype(np.uint8) for row in rows])\n",
    "y = np.array([row.tumor_score for row in rows]).astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x.dtype, x.shape, y.dtype, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.expand_dims(y, -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_data(df):\n",
    "  \"\"\"\n",
    "  Extract data from a PySpark DataFrame into a NumPy array.\n",
    "  \"\"\"\n",
    "  rows = df.collect()\n",
    "  x = np.array([row.sample.toArray().astype(np.uint8) for row in rows])\n",
    "  y = np.array([row.tumor_score for row in rows]).astype(np.uint8)\n",
    "  return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_val, y_val = extract_data(val_df_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_val.dtype, x_val.shape, y_val.dtype, y_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create convnet model graph\n",
    "Create network:\n",
    "  conv1 -> relu1 -> pool1 -> conv2 -> relu2 -> pool2 -> conv3 -> relu3 -> pool3 -> affine1 -> relu1 -> affine2 -> softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1. Add TensorBoard summaries and track.\n",
    "# 2. Plug into larger dataset.\n",
    "# 3. Run on cluster.\n",
    "# 4. Explore saving to TFRecord format, then reading from files shared on DFS (gfs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Hyperparams & Settings\n",
    "classes = CLASSES\n",
    "features = FEATURES\n",
    "C = CHANNELS  # Number of input channels (dimensionality of input depth)\n",
    "Hin = SIZE  # Input height\n",
    "Win = SIZE  # Input width\n",
    "Hf = 3  # conv filter height\n",
    "Wf = 3  # conv filter width\n",
    "Hfp = 2  # pool filter height\n",
    "Wfp = 2  # pool filter width\n",
    "stride = 1  # conv stride\n",
    "pstride = 2  # pool stride\n",
    "pad = 1  # For same dimensions, (Hf - stride) / 2\n",
    "F1 = 32  # num conv filters in conv1\n",
    "F2 = 32  # num conv filters in conv2\n",
    "F3 = 32  # num conv filters in conv3\n",
    "N1 = 512  # num nodes in affine1\n",
    "lr = 5e-4 # learning rate\n",
    "\n",
    "# Inputs\n",
    "with tf.name_scope(\"input\") as scope:\n",
    "  x = tf.placeholder(tf.uint8, [None, features], name=\"x\")\n",
    "  x_scaled = x / 255 * 2 - 1  # scale between [-1,1]\n",
    "  x_image = tf.transpose(tf.reshape(x_scaled, [-1, C, Hin, Win]), perm=[0,2,3,1])  # shape (N,H,W,C)\n",
    "  y_ = tf.placeholder(tf.uint8, [None,], name=\"y_\")\n",
    "  y_one_hot = tf.one_hot(tf.cast(y_, tf.int32) - 1, classes, dtype=tf.int32)  # or use sparse cross entropy\n",
    "  tf.summary.image(\"x\", x_image)\n",
    "  tf.summary.histogram(\"x\", x)\n",
    "  tf.summary.histogram(\"x_scaled\", x_scaled)\n",
    "  tf.summary.histogram(\"y\", y_)\n",
    "\n",
    "# Conv layer 1: conv1 -> relu1 -> pool1\n",
    "with tf.name_scope(\"conv1\") as scope:\n",
    "  W = tf.Variable(tf.random_normal([Hf, Wf, C, F1]) * np.sqrt(2.0/(Hf*Wf*C)), name=\"W\")\n",
    "  b = tf.Variable(tf.zeros([F1]), name=\"b\")\n",
    "  conv = tf.nn.conv2d(x_image, W, [1,stride,stride,1], padding=\"SAME\") + b\n",
    "  relu = tf.nn.relu(conv)\n",
    "  pool = tf.nn.max_pool(relu, ksize=[1,Hfp,Wfp,1], strides=[1,pstride,pstride,1], padding=\"SAME\")\n",
    "  tf.summary.image(\"conv1\", tf.transpose(W, [3,0,1,2]), max_outputs=F1)  # transpose to [N,H,W,C]\n",
    "\n",
    "# Conv layer 2: conv2 -> relu2 -> pool2\n",
    "with tf.name_scope(\"conv2\") as scope:\n",
    "  W = tf.Variable(tf.random_normal([Hf, Wf, F1, F2]) * np.sqrt(2.0/(Hf*Wf*F1)), name=\"W\")\n",
    "  b = tf.Variable(tf.zeros([F2]), name=\"b\")\n",
    "  conv = tf.nn.conv2d(pool, W, [1,stride,stride,1], padding=\"SAME\") + b\n",
    "  relu = tf.nn.relu(conv)\n",
    "  pool = tf.nn.max_pool(relu, ksize=[1,Hfp,Wfp,1], strides=[1,pstride,pstride,1], padding=\"SAME\")\n",
    "\n",
    "# Conv layer 3: conv3 -> relu3 -> pool3\n",
    "with tf.name_scope(\"conv3\") as scope:\n",
    "  W = tf.Variable(tf.random_normal([Hf, Wf, F2, F3]) * np.sqrt(2.0/(Hf*Wf*F2)), name=\"W\")\n",
    "  b = tf.Variable(tf.zeros([F3]), name=\"b\")\n",
    "  conv = tf.nn.conv2d(pool, W, [1,stride,stride,1], padding=\"SAME\") + b\n",
    "  relu = tf.nn.relu(conv)\n",
    "  pool = tf.nn.max_pool(relu, ksize=[1,Hfp,Wfp,1], strides=[1,pstride,pstride,1], padding=\"SAME\")\n",
    "\n",
    "# Affine layer 1:  affine1 -> relu1 -> dropout\n",
    "with tf.name_scope(\"affine1\") as scope:\n",
    "  D = int(F3*(Hin/2**3)*(Win/2**3))\n",
    "  W = tf.Variable(tf.random_normal([D,N1]) * np.sqrt(2.0/D), name=\"W\")\n",
    "  b = tf.Variable(tf.zeros([N1]), name=\"b\")\n",
    "  affine = tf.matmul(tf.reshape(pool, [-1,D]), W) + b\n",
    "  relu = tf.nn.relu(affine)\n",
    "  keep_prob = tf.placeholder(tf.float32, name=\"keep_prob\")\n",
    "  dropout = tf.nn.dropout(relu, keep_prob)\n",
    "\n",
    "# Affine layer 2:  affine2 -> softmax\n",
    "with tf.name_scope(\"affine2\") as scope:\n",
    "  W = tf.Variable(tf.random_normal([N1,classes]) * np.sqrt(2.0/N1), name=\"W\")\n",
    "  b = tf.Variable(tf.zeros([classes]), name=\"b\")\n",
    "  logits = tf.matmul(dropout, W) + b\n",
    "  probs = tf.nn.softmax(logits)\n",
    "  tf.summary.histogram(\"logits\", logits)\n",
    "  tf.summary.histogram(\"probs\", probs)\n",
    "\n",
    "# Loss\n",
    "with tf.name_scope(\"loss\") as scope:\n",
    "  cross_entropy_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, y_one_hot))\n",
    "  tf.summary.scalar(\"loss\", cross_entropy_loss)\n",
    "\n",
    "# Train\n",
    "#train_step = tf.train.GradientDescentOptimizer(lr).minimize(cross_entropy_loss)\n",
    "#train_step = tf.train.AdamOptimizer(lr).minimize(cross_entropy_loss)\n",
    "train_step = tf.train.MomentumOptimizer(lr, 0.9, use_nesterov=True).minimize(cross_entropy_loss)\n",
    "\n",
    "# Eval metrics\n",
    "with tf.name_scope(\"eval\") as scope:\n",
    "  correct_pred = tf.equal(tf.argmax(logits,1), tf.argmax(y_one_hot,1))\n",
    "  accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "  tf.summary.scalar(\"accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "x_scaled.dtype, x_image.dtype, y_one_hot.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Run `tensorboard --logdir=tf_logs --host=localhost --debug --reload_interval 5`\n",
    "# Note: look at stderr for device placement logs\n",
    "with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n",
    "  # Summaries\n",
    "  log_dir = \"tf_logs\"\n",
    "  summary_op = tf.summary.merge_all()\n",
    "  train_writer = tf.train.SummaryWriter(log_dir + \"/train\", sess.graph)\n",
    "  val_writer = tf.train.SummaryWriter(log_dir + \"/val\")\n",
    "  \n",
    "  # Data Gen\n",
    "  batch_size = 64\n",
    "  capacity = 1000\n",
    "  train_generator = gen_batch(train_df.rdd, batch_size)\n",
    "  # ~20% speedup by using threaded queueing generator\n",
    "  train_generator = QueueingGenerator(train_generator, capacity)\n",
    "  # TODO: Setup batch loop over val dataset\n",
    "  x_val, y_val = extract_data(val_df_sample)\n",
    "\n",
    "  # Checkpoint saver\n",
    "  saver = tf.train.Saver(tf.global_variables())\n",
    "\n",
    "  # Train\n",
    "  sess.run(tf.global_variables_initializer())\n",
    "  #steps = 100\n",
    "  epochs = 1\n",
    "  steps = round(epochs * train_df.count() / batch_size)\n",
    "  steps = 200\n",
    "  for i in range(steps):\n",
    "    xs, ys = next(train_generator)\n",
    "    _ = sess.run([train_step], feed_dict={x: xs, y_:ys, keep_prob:0.5})\n",
    "    if i % 100 == 0:\n",
    "      # train stats\n",
    "      summary, train_loss, train_acc = sess.run([summary_op, cross_entropy_loss, accuracy], feed_dict={x: xs, y_:ys, keep_prob:0.5})\n",
    "      train_writer.add_summary(summary, i)\n",
    "      # val stats\n",
    "      summary, val_loss, val_acc = sess.run([summary_op, cross_entropy_loss, accuracy], feed_dict={x: x_val, y_:y_val, keep_prob:1})\n",
    "      val_writer.add_summary(summary, i)\n",
    "      print(\"Iter: {}, \\t Train Accuracy, Loss: {:.4f},{:.4f} \\t Val Accuracy, Loss: {:.4f},{:.4f}\".format(i, train_acc, train_loss, val_acc, val_loss))\n",
    "      #print(\"Iter: {}, \\t Train Accuracy: {:.4f}\".format(i, train_acc))\n",
    "      if i % 1000 == 0 or (i+1) == steps:\n",
    "        # Save model periodically\n",
    "        # TODO: Only save best `n` models.\n",
    "        checkpoint_path = os.path.join(\"models\", \"{}.ckpt\".format(val_acc))\n",
    "        saver.save(sess, checkpoint_path, global_step=i)\n",
    "  train_writer.flush()  # Make sure everything is written before exiting\n",
    "  val_writer.flush()  # Make sure everything is written before exiting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Setup VGG16 pretrained model with new input & output layers.\n",
    "2. Train new output layers (all others frozen).\n",
    "3. Fine tune additional layers.\n",
    "4. Profit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Look into labels being set as float64 instead of float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Clear out any existing Keras logs and model checkpoints\n",
    "log_dir = os.path.join(\"tf_logs\", \"keras\", \"vgg\")\n",
    "model_dir = os.path.join(\"models\", \"keras\", \"vgg\")\n",
    "for path in [log_dir, model_dir]:\n",
    "  if os.path.exists(path):\n",
    "    #os.rmdir(path)  # fails if directory is not empty\n",
    "    shutil.rmtree(path)\n",
    "\n",
    "# Reset any current Keras session\n",
    "import keras.backend as K\n",
    "K.clear_session()  # reset TensorFlow session for iterative work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras.layers import Dense, Flatten, Input, Permute, Reshape\n",
    "from keras.models import Model\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Clean these up\n",
    "def to_categorical(y, classes):\n",
    "  # Avoid cast to float64 as done in keras.utils.np_utils.to_categorical\n",
    "  n = len(y)\n",
    "  Y = np.zeros((n, classes), dtype=np.int32)\n",
    "  Y[np.arange(n), y] = 1\n",
    "  #Y = np.eye(classes)[y]\n",
    "  return Y\n",
    "\n",
    "def gen_preprocessed_batch(batch_generator):\n",
    "  for xs, ys in batch_generator:\n",
    "    xs = (xs.reshape((-1,CHANNELS,SIZE,SIZE))  # shape (N,C,H,W)\n",
    "            .transpose((0,2,3,1))  # shape (N,H,W,C)\n",
    "            .astype(np.float32))\n",
    "    yield preprocess_input(xs), to_categorical(ys-1, CLASSES) #, ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create train and validation iterators.\n",
    "batch_size = 32\n",
    "train_generator = gen_preprocessed_batch(gen_batch(train_df.rdd, batch_size))\n",
    "val_generator = gen_preprocessed_batch(gen_batch(val_df.rdd, batch_size))\n",
    "val_sample_generator = gen_preprocessed_batch(gen_batch(val_df_sample.rdd, batch_size))\n",
    "\n",
    "## ~20% speedup by using threaded queueing generator\n",
    "# capacity = 1000\n",
    "# train_generator = QueueingGenerator(train_generator, capacity)\n",
    "# val_generator = QueueingGenerator(val_generator, capacity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x, y, y_orig = next(train_generator)\n",
    "x.shape, x.dtype, y.shape, y.dtype, y_orig.shape, y_orig.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y[1], y_orig[1], y[20], y_orig[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# inputs = Input(shape=(FEATURES,))  # shape (N,C*H*W)\n",
    "# inputs_reshaped = Reshape((C,SIZE,SIZE))(inputs)  # shape (N,C,H,W)\n",
    "# inputs_transposed = Permute((2,3,1))(inputs_reshaped)  # shape (N,H,W,C)\n",
    "\n",
    "# VGG16 without the fully-connected layers at the end\n",
    "# base_model = VGG16(weights=\"imagenet\", include_top=False, \n",
    "#                    input_tensor=inputs_transposed, input_shape=(SIZE,SIZE,C))\n",
    "base_model = VGG16(weights=\"imagenet\", include_top=False, input_shape=(SIZE,SIZE,CHANNELS))\n",
    "\n",
    "# New fully-connected layers for breast cancer problem\n",
    "x = base_model.output\n",
    "x = Flatten()(x)\n",
    "x = Dense(256, activation=\"relu\")(x)\n",
    "predictions = Dense(CLASSES, activation=\"softmax\")(x)\n",
    "\n",
    "# Create overall model\n",
    "# model = Model(input=inputs, output=predictions)\n",
    "model = Model(input=base_model.input, output=predictions)\n",
    "\n",
    "# Freeze all layers except new ones\n",
    "for layer in base_model.layers:\n",
    "  layer.trainable = False\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=['accuracy'])\n",
    "\n",
    "# Setup training callbacks\n",
    "log_dir = os.path.join(\"tf_logs\", \"keras\", \"vgg\")\n",
    "model_dir = os.path.join(\"models\", \"keras\", \"vgg\")\n",
    "model_filename = os.path.join(model_dir, \"{val_loss:.2f}-{epoch:02d}.hdf5\")\n",
    "for path in [log_dir, model_dir]:\n",
    "  if not os.path.exists(path):\n",
    "    os.mkdir(path)\n",
    "tensorboard = TensorBoard(log_dir=log_dir)  #, histogram_freq=1, write_images=True)\n",
    "checkpointer = ModelCheckpoint(model_filename)\n",
    "callbacks = [tensorboard, checkpointer]\n",
    "\n",
    "# Train these new layers at the end\n",
    "train_samples = math.ceil(train_df.count()/batch_size) * batch_size  #10000\n",
    "val_samples = math.ceil(val_df.count()/batch_size) * batch_size  #2000\n",
    "val_sample_samples = math.ceil(val_sample_df.count()/batch_size) * batch_size  #2000\n",
    "epochs = 1\n",
    "model.fit_generator(train_generator, samples_per_epoch=train_samples, nb_epoch=epochs,\n",
    "                    validation_data=val_generator, nb_val_samples=val_samples,\n",
    "                    max_q_size=10000, # vary the queue size\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val_sample_samples = math.ceil(val_df_sample.count()/batch_size) * batch_size  #2000\n",
    "val_sample_samples, val_df_sample.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.evaluate_generator(val_sample_generator, val_samples=val_sample_samples, max_q_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.metrics_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: Try caching RDD in generator function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Read in 256x256x3 dataframe.\n",
    "2. Create random crops to 224x224x3.\n",
    "3. Setup ResNet50 pretrained model with new input & output layers.\n",
    "4. Train new output layers (all others frozen).\n",
    "5. Fine tune additional layers.\n",
    "6. Profit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Setup model and log directories\n",
    "log_dir = os.path.join(\"tf_logs\", \"keras\", \"resnet50\")\n",
    "model_dir = os.path.join(\"models\", \"keras\", \"resnet50\")\n",
    "for path in [log_dir, model_dir]:\n",
    "  if not os.path.exists(path):\n",
    "    #os.mkdir(path)\n",
    "    os.makedirs(path)  # make all intermediate dirs"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Clear out any existing Keras logs and model checkpoints\n",
    "for path in [log_dir, model_dir]:\n",
    "  if os.path.exists(path):\n",
    "    #os.rmdir(path)  # fails if directory is not empty\n",
    "    shutil.rmtree(path)\n",
    "\n",
    "# Reset any current Keras session\n",
    "import keras.backend as K\n",
    "K.clear_session()  # reset TensorFlow session for iterative work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras.layers import Dense, Flatten, Input, Permute, Reshape\n",
    "from keras.models import Model\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Clean these up\n",
    "def to_categorical(y, classes):\n",
    "  # Avoid cast to float64 as done in keras.utils.np_utils.to_categorical\n",
    "  n = len(y)\n",
    "  Y = np.zeros((n, classes), dtype=np.int32)\n",
    "  Y[np.arange(n), y] = 1\n",
    "  #Y = np.eye(classes)[y]\n",
    "  return Y\n",
    "\n",
    "def gen_preprocessed_batch(batch_generator):\n",
    "  for xs, ys in batch_generator:\n",
    "    xs = (xs.reshape((-1,CHANNELS,SIZE,SIZE))  # shape (N,C,H,W)\n",
    "            .transpose((0,2,3,1))  # shape (N,H,W,C)\n",
    "            .astype(np.float32))\n",
    "    yield preprocess_input(xs), to_categorical(ys-1, CLASSES) #, ys"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Create train and validation iterators.\n",
    "batch_size = 32\n",
    "train_generator = gen_preprocessed_batch(gen_batch(train_df.rdd, batch_size))\n",
    "val_generator = gen_preprocessed_batch(gen_batch(val_df.rdd, batch_size))\n",
    "val_sample_generator = gen_preprocessed_batch(gen_batch(val_df_sample.rdd, batch_size))\n",
    "\n",
    "## ~20% speedup by using threaded queueing generator\n",
    "# capacity = 1000\n",
    "# train_generator = QueueingGenerator(train_generator, capacity)\n",
    "# val_generator = QueueingGenerator(val_generator, capacity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create train and validation iterators.\n",
    "batch_size = 32\n",
    "# train_generator = gen_preprocessed_batch(gen_batch2(batch_queue))\n",
    "train_generator = gen_preprocessed_batch(gen_batch2(row_queue))\n",
    "val_generator = gen_preprocessed_batch(gen_batch(val_df.rdd, batch_size))\n",
    "# val_sample_generator = gen_preprocessed_batch(gen_batch2(val_df_sample.rdd, batch_size))\n",
    "\n",
    "## ~20% speedup by using threaded queueing generator\n",
    "# capacity = 1000\n",
    "# train_generator = QueueingGenerator(train_generator, capacity)\n",
    "# val_generator = QueueingGenerator(val_generator, capacity)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "x, y, y_orig = next(train_generator)\n",
    "x.shape, x.dtype, y.shape, y.dtype, y_orig.shape, y_orig.dtype"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "y[1], y_orig[1], y[20], y_orig[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# inputs = Input(shape=(FEATURES,))  # shape (N,C*H*W)\n",
    "# inputs_reshaped = Reshape((C,SIZE,SIZE))(inputs)  # shape (N,C,H,W)\n",
    "# inputs_transposed = Permute((2,3,1))(inputs_reshaped)  # shape (N,H,W,C)\n",
    "\n",
    "# VGG16 without the fully-connected layers at the end\n",
    "# base_model = VGG16(weights=\"imagenet\", include_top=False, \n",
    "#                    input_tensor=inputs_transposed, input_shape=(SIZE,SIZE,C))\n",
    "base_model = ResNet50(weights=\"imagenet\", include_top=False, input_shape=(SIZE,SIZE,CHANNELS))\n",
    "\n",
    "# New fully-connected layers for breast cancer problem\n",
    "x = base_model.output\n",
    "x = Flatten()(x)\n",
    "x = Dense(1024, activation=\"relu\")(x)\n",
    "predictions = Dense(CLASSES, activation=\"softmax\")(x)\n",
    "\n",
    "# Create overall model\n",
    "# model = Model(input=inputs, output=predictions)\n",
    "model = Model(input=base_model.input, output=predictions)\n",
    "\n",
    "# Freeze all layers except new ones\n",
    "for layer in base_model.layers:\n",
    "  layer.trainable = False\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=['accuracy'])\n",
    "\n",
    "# Setup training callbacks\n",
    "# model_filename = os.path.join(model_dir, \"{val_loss:.2f}-{epoch:02d}.hdf5\")\n",
    "tensorboard = TensorBoard(log_dir=log_dir)  #, histogram_freq=1, write_images=True)\n",
    "# checkpointer = ModelCheckpoint(model_filename)\n",
    "callbacks = [tensorboard] #, checkpointer]\n",
    "\n",
    "# Train these new layers at the end\n",
    "# TODO: Update count logic\n",
    "train_samples = 10000 #math.ceil(tc/batch_size) * batch_size  #10000\n",
    "val_samples = 2000 #vc  #2000 #math.ceil(val_df.count()/batch_size) * batch_size  #2000\n",
    "epochs = 5\n",
    "model.fit_generator(train_generator, samples_per_epoch=train_samples, nb_epoch=epochs,\n",
    "#                     validation_data=val_generator, nb_val_samples=val_samples,\n",
    "                    max_q_size=10000, # vary the queue size\n",
    "                    callbacks=callbacks,\n",
    "                    nb_worker=4, pickle_safe=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Cleanup input queues: Use processes with Keras, replace batch_queue with Keras,\n",
    "# increase partition queue size, add function to create generators for any df, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Add queueing val generator back to training, maybe with less threads (although it will block)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Add `stop` method/events to Threads/Processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Monitor size of input queues with callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Expand the size of the Dense layer, and perhaps replace Flatten with GlobalAveragePooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Determine what the class ratio is in the first 10000 train samples using `limit(10000)`; do the same with val"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "val_sample_samples = math.ceil(val_df_sample.count()/batch_size) * batch_size  #2000\n",
    "val_sample_samples, val_df_sample.count()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "model.evaluate_generator(val_sample_generator, val_samples=val_sample_samples, max_q_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# val_generator = gen_preprocessed_batch(gen_batch(val_df.rdd, batch_size))\n",
    "model.evaluate_generator(val_generator, val_samples=vc, max_q_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
