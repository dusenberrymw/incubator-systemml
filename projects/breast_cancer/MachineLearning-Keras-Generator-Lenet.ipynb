{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import math\n",
    "import multiprocessing as mp\n",
    "import os\n",
    "import queue\n",
    "import shutil\n",
    "import threading\n",
    "\n",
    "from keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras.initializations import get_fans, normal\n",
    "from keras.layers import Activation, Convolution2D, Dense, Dropout, Flatten, Input, MaxPooling2D, Permute, Reshape\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD\n",
    "# from keras.utils.np_utils import to_categorical\n",
    "import keras.backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from breast_cancer import input_data\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"1\"\n",
    "# c = tf.ConfigProto()\n",
    "# c.gpu_options.visible_device_list=\"0\"\n",
    "# sess = tf.Session(config=c)\n",
    "# K.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Read in train & val data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "SIZE = 256\n",
    "CHANNELS = 3\n",
    "FEATURES = SIZE * SIZE * CHANNELS\n",
    "CLASSES = 3\n",
    "p = 0.01\n",
    "train_df, val_df = input_data.read_train_val_data(spark, SIZE, CHANNELS, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tc = train_df.count()\n",
    "vc = val_df.count()\n",
    "print(tc, vc)\n",
    "print(train_df.rdd.getNumPartitions(), val_df.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Compute image channel means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "means = input_data.compute_channel_means(train_df, CHANNELS, SIZE)\n",
    "print(means.shape)\n",
    "print(means)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Generate class weights for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class_weights = input_data.gen_class_weights(train_df)\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Create asynchronous queuing batch generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create train & val generators\n",
    "batch_size = 32\n",
    "train_generator_orig, train_ps, train_queues, train_stop_event = input_data.create_batch_generator(train_df.rdd, batch_size=batch_size)\n",
    "val_generator_orig, val_ps, val_queues, val_stop_event = input_data.create_batch_generator(val_df.rdd, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# # Print queue sizes (for debugging)\n",
    "# for q in train_queues + val_queues:\n",
    "#   print(q.qsize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## \"LeNet\"-like Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Setup model and log directories\n",
    "log_dir = os.path.join(\"tf_logs\", \"keras\", \"lenet\")\n",
    "model_dir = os.path.join(\"models\", \"keras\", \"lenet\")\n",
    "for path in [log_dir, model_dir]:\n",
    "  if not os.path.exists(path):\n",
    "    os.makedirs(path)  # make all intermediate dirs, unlike `os.mkdir(path)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# # Clear out any existing Keras logs and model checkpoints\n",
    "# for path in [log_dir, model_dir]:\n",
    "#   if os.path.exists(path):\n",
    "#     #os.rmdir(path)  # fails if directory is not empty\n",
    "#     shutil.rmtree(path)\n",
    "\n",
    "# # Reset any current Keras session\n",
    "# import keras.backend as K\n",
    "# K.clear_session()  # reset TensorFlow session for iterative work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Preprocess with slide image means\n",
    "def preprocess_input(x, dim_ordering='default'):\n",
    "    \"\"\"Preprocesses a tensor encoding a batch of images.\n",
    "    # Arguments\n",
    "        x: input Numpy tensor, 4D.\n",
    "    # Returns\n",
    "        Preprocessed tensor.\n",
    "    \"\"\"\n",
    "    if dim_ordering == 'default':\n",
    "        dim_ordering = K.image_dim_ordering()\n",
    "    assert dim_ordering in {'tf', 'th'}\n",
    "\n",
    "    if dim_ordering == 'th':\n",
    "        # 'RGB'->'BGR'\n",
    "        x = x[:, ::-1, :, :]\n",
    "        # Zero-center by mean pixel\n",
    "        x[:, 0, :, :] -= 103.939\n",
    "        x[:, 1, :, :] -= 116.779\n",
    "        x[:, 2, :, :] -= 123.68\n",
    "    else:\n",
    "        # 'RGB'->'BGR'\n",
    "        x = x[:, :, :, ::-1]\n",
    "        # Zero-center by mean pixel\n",
    "        # `means` is stored in RGB, but we need BGR\n",
    "        x[:, :, :, 0] -= means[2]  #103.939\n",
    "        x[:, :, :, 1] -= means[0]  #116.779\n",
    "        x[:, :, :, 2] -= means[1]  #123.68\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def to_categorical(y, classes):\n",
    "  # Avoid cast to float64 as done in keras.utils.np_utils.to_categorical\n",
    "  n = len(y)\n",
    "  Y = np.zeros((n, classes), dtype=np.int32)\n",
    "  Y[np.arange(n), y] = 1\n",
    "  return Y\n",
    "\n",
    "# TODO: Clean this up -- remove access to global variables\n",
    "def gen_preprocessed_batch(batch_generator):\n",
    "  for xs, ys in batch_generator:\n",
    "    xs = (xs.reshape((-1,CHANNELS,SIZE,SIZE))  # shape (N,C,H,W)\n",
    "            .transpose((0,2,3,1))  # shape (N,H,W,C)\n",
    "            .astype(np.float32))\n",
    "    yield preprocess_input(xs), to_categorical(ys-1, CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create train & val preprocessed generators\n",
    "train_generator = gen_preprocessed_batch(train_generator_orig)\n",
    "val_generator = gen_preprocessed_batch(val_generator_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Setup training callbacks\n",
    "model_filename = os.path.join(model_dir, \"{val_loss:.2f}-{epoch:02d}.hdf5\")\n",
    "# Careful, TensorBoard callback could OOM with large validation set\n",
    "tensorboard = TensorBoard(log_dir=log_dir)  #, histogram_freq=1, write_images=True)\n",
    "checkpointer = ModelCheckpoint(model_filename)\n",
    "callbacks = [tensorboard, checkpointer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Custom final dense layer initializer\n",
    "def my_init(shape, name=None, dim_ordering='tf'):\n",
    "    \"\"\"Guassian scaled by sqrt(1/fan_in)\"\"\"\n",
    "    fan_in, fan_out = get_fans(shape, dim_ordering=dim_ordering)\n",
    "    s = np.sqrt(1. / fan_in)\n",
    "    return normal(shape, s, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create a \"LeNet\"-like model\n",
    "f = 3\n",
    "inputs = Input(shape=(SIZE,SIZE,CHANNELS))\n",
    "x = Convolution2D(32, f, f, init=\"he_normal\", border_mode=\"same\", activation=\"relu\")(inputs)\n",
    "x = MaxPooling2D()(x)\n",
    "x = Convolution2D(64, f, f, init=\"he_normal\", border_mode=\"same\", activation=\"relu\")(x)\n",
    "x = MaxPooling2D()(x)\n",
    "x = Convolution2D(128, f, f, init=\"he_normal\", border_mode=\"same\", activation=\"relu\")(x)\n",
    "x = MaxPooling2D()(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(256, init=\"he_normal\", W_regularizer='l2', activation=\"relu\")(x)\n",
    "# x = Dropout(0.5)(x)\n",
    "predictions = Dense(CLASSES, init=my_init, activation=\"softmax\")(x)\n",
    "\n",
    "# Create overall model\n",
    "model = Model(input=inputs, output=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Compile model\n",
    "# optim = SGD(lr=0.01, momentum=0.5, decay=0.0, nesterov=True)\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Train these new layers at the end\n",
    "train_samples = math.ceil(tc/batch_size) * batch_size\n",
    "val_samples = math.ceil(vc/batch_size) * batch_size\n",
    "epochs = 5\n",
    "model.fit_generator(train_generator, samples_per_epoch=train_samples, nb_epoch=epochs,\n",
    "                    validation_data=val_generator, nb_val_samples=val_samples,\n",
    "                    class_weight=class_weights,\n",
    "#                     max_q_size=10000, # vary the queue size\n",
    "#                     callbacks=callbacks,\n",
    "                    nb_worker=1, pickle_safe=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Evaluate model on validation set\n",
    "raw_metrics = model.evaluate_generator(val_generator, val_samples=val_samples)\n",
    "metrics = dict(zip(model.metrics_names, raw_metrics))\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Save model\n",
    "filename = \"{acc:.5}_acc_{loss:.5}_loss_model.hdf5\".format(**metrics)\n",
    "model.save(os.path.join(model_dir, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# TODO: Monitor size of input queues with callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# TODO: Expand the size of the Dense layer, and perhaps replace Flatten with GlobalAveragePooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 + Spark 2.x + SystemML",
   "language": "python",
   "name": "pyspark3_2.x"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
