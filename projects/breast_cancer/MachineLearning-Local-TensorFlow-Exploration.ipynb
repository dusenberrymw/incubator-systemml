{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in train & val data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df = sqlContext.read.load(\"data/train_100_grayscale.parquet\")\n",
    "val_df = sqlContext.read.load(\"data/val_100_grayscale.parquet\")\n",
    "train_df, val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tc = train_df.count()\n",
    "vc = val_df.count()\n",
    "tc, vc  # 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df.select(\"tumor_score\").groupBy(\"tumor_score\").count().show()\n",
    "val_df.select(\"tumor_score\").groupBy(\"tumor_score\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "C = 1\n",
    "SIZE = train_df.first().sample.toArray().shape[0]\n",
    "SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test out local iterator of RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iterator = train_df.rdd.toLocalIterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b = next(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(b)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t = np.array([b.sample.values, b.sample.values])\n",
    "u = np.array([b.tumor_score, b.tumor_score]).astype(np.int8)\n",
    "t.shape, t.dtype, u.shape, u.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create batch generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iterator = train_df.rdd.toLocalIterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create generator that cycles through the data and yields a batch at a time,\n",
    "# reinitializing the iterator as needed to continue yielding batches.\n",
    "# Look at TensorFlow MNIST batch generator.\n",
    "# Should be able to pass in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pseudocode\n",
    "def gen_batch(rdd, batch_size=50):\n",
    "  iterator = rdd.toLocalIterator()\n",
    "  while True:\n",
    "    features = []\n",
    "    labels = []\n",
    "    for i in range(batch_size):\n",
    "      # Generate batch\n",
    "      try:\n",
    "        row = next(iterator)\n",
    "      except StopIteration:\n",
    "        # Restart iterator\n",
    "        iterator = rdd.toLocalIterator()\n",
    "        row = next(iterator)\n",
    "      features.append(row.sample.values)\n",
    "      labels.append(row.tumor_score)\n",
    "    x_batch = np.array(features).astype(np.float32)\n",
    "    y_batch = np.array(labels).astype(np.int32)\n",
    "    yield x_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "generator = gen_batch(train_df.rdd, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x, y = next(generator)\n",
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x.shape, x.dtype, y.shape, y.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rows = val_df.take(3)  #.collect()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.array([row.sample.toArray().astype(np.float32) for row in rows])\n",
    "y = np.array([row.tumor_score for row in rows])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x.dtype, x.shape, y.dtype, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_data(df):\n",
    "  rows = df.collect()\n",
    "  x = np.array([row.sample.toArray().astype(np.float32) for row in rows])\n",
    "  y = np.array([row.tumor_score for row in rows])\n",
    "  return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create softmax model graph"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "classes = 3\n",
    "x = tf.placeholder(tf.float32, [None, 65536])\n",
    "W = tf.Variable(tf.truncated_normal([65536, classes], stddev=0.01))\n",
    "b = tf.Variable(tf.zeros([classes]))\n",
    "y = tf.matmul(x, W) + b\n",
    "\n",
    "y_ = tf.placeholder(tf.int32, [None, ])\n",
    "y_one_hot = tf.one_hot(y_-1, classes)\n",
    "\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y, y_one_hot))\n",
    "train_step = tf.train.GradientDescentOptimizer(0.1).minimize(cross_entropy)\n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(y,1), tf.argmax(y_one_hot,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with tf.Session() as sess:\n",
    "  generator = gen_batch(train_df.rdd, 32)\n",
    "  sess.run(tf.initialize_all_variables())\n",
    "  steps = 100\n",
    "  for i in range(steps):\n",
    "    xs, ys = next(generator)\n",
    "    _, loss, acc = sess.run([train_step, cross_entropy, accuracy], feed_dict={x: xs, y_:ys})\n",
    "    print(\"Step: {:5} \\t Loss: {:10.4f} \\t Acc: {}\".format(i, loss, acc))\n",
    "  #y_out = sess.run(y_one_hot, feed_dict={y_: ys})\n",
    "#y_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create convnet model graph\n",
    "Create network:\n",
    "  conv1 -> relu1 -> pool1 -> conv2 -> relu2 -> pool2 -> conv3 -> relu3 -> pool3 -> affine1 -> relu1 -> affine2 -> softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Hyperparams & Settings\n",
    "classes = 3\n",
    "features = 65536\n",
    "C = 1  # Number of input channels (dimensionality of input depth)\n",
    "Hin = 256  # Input height\n",
    "Win = 256  # Input width\n",
    "Hf = 3  # conv filter height\n",
    "Wf = 3  # conv filter width\n",
    "Hfp = 2  # pool filter height\n",
    "Wfp = 2  # pool filter width\n",
    "stride = 1  # conv stride\n",
    "pstride = 2  # pool stride\n",
    "pad = 1  # For same dimensions, (Hf - stride) / 2\n",
    "F1 = 32  # num conv filters in conv1\n",
    "F2 = 32  # num conv filters in conv2\n",
    "F3 = 32  # num conv filters in conv3\n",
    "N1 = 512  # num nodes in affine1\n",
    "lr = 1e-4 # learning rate\n",
    "\n",
    "# Inputs\n",
    "with tf.name_scope(\"input\") as scope:\n",
    "  x = tf.placeholder(tf.float32, [None, features], name=\"x\")\n",
    "  x_image = tf.transpose(tf.reshape(x, [-1, C, Hin, Win]), perm=[0,2,3,1])  # shape (N,H,W,C)\n",
    "  y_ = tf.placeholder(tf.int64, [None, ], name=\"y_\")\n",
    "  y_one_hot = tf.one_hot(y_-1, classes)  # or use sparse cross entropy\n",
    "  tf.summary.image(\"x\", x_image)\n",
    "  tf.summary.histogram(\"y\", y_)\n",
    "\n",
    "# Conv layer 1: conv1 -> relu1 -> pool1\n",
    "with tf.name_scope(\"conv1\") as scope:\n",
    "  W = tf.Variable(tf.random_normal([Hf, Wf, C, F1]) * np.sqrt(2.0/(Hf*Wf*C)), name=\"W\")\n",
    "  b = tf.Variable(tf.zeros([F1]), name=\"b\")\n",
    "  conv = tf.nn.conv2d(x_image, W, [1,stride,stride,1], padding=\"SAME\") + b\n",
    "  relu = tf.nn.relu(conv)\n",
    "  pool = tf.nn.max_pool(relu, ksize=[1,Hfp,Wfp,1], strides=[1,pstride,pstride,1], padding=\"SAME\")\n",
    "  tf.summary.image(\"conv1\", tf.transpose(W, [3,0,1,2]), max_outputs=F1)  # transpose to [N,H,W,C]\n",
    "\n",
    "# Conv layer 2: conv2 -> relu2 -> pool2\n",
    "with tf.name_scope(\"conv2\") as scope:\n",
    "  W = tf.Variable(tf.random_normal([Hf, Wf, F1, F2]) * np.sqrt(2.0/(Hf*Wf*F1)), name=\"W\")\n",
    "  b = tf.Variable(tf.zeros([F2]), name=\"b\")\n",
    "  conv = tf.nn.conv2d(pool, W, [1,stride,stride,1], padding=\"SAME\") + b\n",
    "  relu = tf.nn.relu(conv)\n",
    "  pool = tf.nn.max_pool(relu, ksize=[1,Hfp,Wfp,1], strides=[1,pstride,pstride,1], padding=\"SAME\")\n",
    "\n",
    "# Conv layer 3: conv3 -> relu3 -> pool3\n",
    "with tf.name_scope(\"conv3\") as scope:\n",
    "  W = tf.Variable(tf.random_normal([Hf, Wf, F2, F3]) * np.sqrt(2.0/(Hf*Wf*F2)), name=\"W\")\n",
    "  b = tf.Variable(tf.zeros([F3]), name=\"b\")\n",
    "  conv = tf.nn.conv2d(pool, W, [1,stride,stride,1], padding=\"SAME\") + b\n",
    "  relu = tf.nn.relu(conv)\n",
    "  pool = tf.nn.max_pool(relu, ksize=[1,Hfp,Wfp,1], strides=[1,pstride,pstride,1], padding=\"SAME\")\n",
    "\n",
    "# Affine layer 1:  affine1 -> relu1 -> dropout\n",
    "with tf.name_scope(\"affine1\") as scope:\n",
    "  D = int(F3*(Hin/2**3)*(Win/2**3))\n",
    "  W = tf.Variable(tf.random_normal([D,N1]) * np.sqrt(2.0/D), name=\"W\")\n",
    "  b = tf.Variable(tf.zeros([N1]), name=\"b\")\n",
    "  affine = tf.matmul(tf.reshape(pool, [-1,D]), W) + b\n",
    "  relu = tf.nn.relu(affine)\n",
    "  keep_prob = tf.placeholder(tf.float32, name=\"keep_prob\")\n",
    "  dropout = tf.nn.dropout(relu, keep_prob)\n",
    "\n",
    "# Affine layer 2:  affine2 -> softmax\n",
    "with tf.name_scope(\"affine2\") as scope:\n",
    "  W = tf.Variable(tf.random_normal([N1,classes]) * np.sqrt(2.0/N1), name=\"W\")\n",
    "  b = tf.Variable(tf.zeros([classes]), name=\"b\")\n",
    "  logits = tf.matmul(dropout, W) + b\n",
    "  probs = tf.nn.softmax(logits)\n",
    "  tf.summary.histogram(\"logits\", logits)\n",
    "  tf.summary.histogram(\"probs\", probs)\n",
    "\n",
    "# Loss\n",
    "with tf.name_scope(\"loss\") as scope:\n",
    "  cross_entropy_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, y_one_hot))\n",
    "  tf.summary.scalar(\"loss\", cross_entropy_loss)\n",
    "\n",
    "# Train\n",
    "# train_step = tf.train.GradientDescentOptimizer(lr).minimize(cross_entropy)\n",
    "train_step = tf.train.AdamOptimizer(lr).minimize(cross_entropy_loss)\n",
    "\n",
    "# Eval metrics\n",
    "with tf.name_scope(\"eval\") as scope:\n",
    "  correct_pred = tf.equal(tf.argmax(logits,1), tf.argmax(y_one_hot,1))\n",
    "  accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "  tf.summary.scalar(\"accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run `tensorboard --logdir=tf_logs --host=localhost --debug --reload_interval 5`\n",
    "with tf.Session() as sess:\n",
    "  # Summaries\n",
    "  log_dir = \"tf_logs\"\n",
    "  summary_op = tf.summary.merge_all()\n",
    "  train_writer = tf.train.SummaryWriter(log_dir + \"/train\", sess.graph)\n",
    "  val_writer = tf.train.SummaryWriter(log_dir + \"/val\")\n",
    "  \n",
    "  # Data Gen\n",
    "  train_generator = gen_batch(train_df.rdd, 64)\n",
    "  x_val, y_val = extract_data(val_df)\n",
    "  \n",
    "  # Train\n",
    "  sess.run(tf.global_variables_initializer())\n",
    "  steps = 100\n",
    "  for i in range(steps):\n",
    "    xs, ys = next(train_generator)\n",
    "    _ = sess.run([train_step], feed_dict={x: xs, y_:ys, keep_prob:0.5})\n",
    "    if i % 10 == 0:\n",
    "      # train stats\n",
    "      summary, train_acc = sess.run([summary_op, accuracy], feed_dict={x: xs, y_:ys, keep_prob:0.5})\n",
    "      train_writer.add_summary(summary, i)\n",
    "      # val stats\n",
    "      summary, val_acc = sess.run([summary_op, accuracy], feed_dict={x: x_val, y_:y_val, keep_prob:0.5})\n",
    "      val_writer.add_summary(summary, i)\n",
    "      #train_writer.flush()  # To force write -- probably slower (usually asynchronous)\n",
    "      #val_writer.flush()  # To force write -- probably slower (usually asynchronous)\n",
    "      print(\"Iter: {}, \\t Train Accuracy: {:.4f}, \\t Val Accuracy: {:.4f}\".format(i, train_acc, val_acc))\n",
    "train_writer.flush()  # Make sure everything is written before exiting\n",
    "val_writer.flush()  # Make sure everything is written before exiting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1. Add TensorBoard summaries and track.\n",
    "# 2. Plug into larger dataset.\n",
    "# 3. Run on cluster.\n",
    "# 4. Explore saving to TFRecord format, then reading from files shared on DFS (gfs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert data to TFRecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.learn.python.learn.datasets import mnist\n",
    "\n",
    "data_sets = mnist.read_data_sets(\"/tmp/data\",\n",
    "                                   dtype=tf.uint8,\n",
    "                                   reshape=False,\n",
    "                                   validation_size=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_sets.train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_sets.train.images.shape, data_sets.train.images.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_sets.train.labels.shape, data_sets.train.labels.dtype"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "data_sets.train.images[0].tostring()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df.rdd.glom().map(len).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df.repartition(3).rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df.repartition(3).rdd.glom().map(len).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df.foreachPartition?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "im = train_df.first().sample.toArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "im.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "im.tostring()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(\"/tmp/tf\"):\n",
    "  os.mkdir(\"/tmp/tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "writer1 = tf.python_io.TFRecordWriter(\"/tmp/tf/testing.tfrecords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "writer2 = tf.python_io.TFRecordWriter(\"/tmp/tf/testing.tfrecords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "writer1.close(), writer2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_batch(rdd, batch_size=50):\n",
    "  iterator = rdd.toLocalIterator()\n",
    "  while True:\n",
    "    features = []\n",
    "    labels = []\n",
    "    for i in range(batch_size):\n",
    "      # Generate batch\n",
    "      try:\n",
    "        row = next(iterator)\n",
    "      except StopIteration:\n",
    "        # Restart iterator\n",
    "        iterator = rdd.toLocalIterator()\n",
    "        row = next(iterator)\n",
    "      features.append(row.sample.values)\n",
    "      labels.append(row.tumor_score)\n",
    "    x_batch = np.array(features).astype(np.float32)\n",
    "    y_batch = np.array(labels).astype(np.int32)\n",
    "    yield x_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "row = train_df.rdd.take(2)[1]\n",
    "#row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "row['__INDEX'], row.slide_num, row.tumor_score, row.molecular_score, row.sample.toArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.train.FloatList?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "\n",
    "def _bytes_feature(value):\n",
    "  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _float_feature(value):\n",
    "  return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "def _int64_feature(value):\n",
    "  return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "def convert(rows, folder):\n",
    "  \"\"\"Convert data to file of tfrecords.\"\"\"\n",
    "  # TODO: A uuid is highly unlikely to overlap, but we implement a truly\n",
    "  # unique identifier.  We could possibly `mapPartitions` to lists, then\n",
    "  # `zipWithUniqueID` on each list, then write each entry, using the ID\n",
    "  # as the filename.\n",
    "  filename = os.path.join(folder, str(uuid.uuid4()) + \".tfrecords\")\n",
    "  writer = tf.python_io.TFRecordWriter(filename)\n",
    "  for row in rows:\n",
    "    example = tf.train.Example(features=tf.train.Features(feature={\n",
    "          '__INDEX': _int64_feature(row['__INDEX']),\n",
    "          'slide_num': _int64_feature(row.slide_num),\n",
    "          'tumor_score': _int64_feature(row.tumor_score),\n",
    "          'molecular_score': _float_feature(row.molecular_score),\n",
    "          'sample': _bytes_feature(row.sample.toArray().astype(np.float32).tostring()) # should use the uint8 DataFrame\n",
    "          # TODO: ENCODE SHAPE\n",
    "        }))\n",
    "    writer.write(example.SerializeToString())\n",
    "  writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "folder = \"data/tf\"\n",
    "if not os.path.exists(folder):\n",
    "  os.mkdir(folder)\n",
    "rows = train_df.take(5)\n",
    "convert(rows, folder)  # side-effect of writing to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "os.listdir(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filename = os.path.join(folder, os.listdir(folder)[0])\n",
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1. Check outputs for correctness\n",
    "# 2. write partitions and check output\n",
    "# 3. Run on cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.python_io.tf_record_iterator?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.parse_single_example?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.TFRecordReader.read?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.train.string_input_producer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.FixedLenFeature?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "filename_queue = tf.train.string_input_producer([filename])\n",
    "\n",
    "reader = tf.TFRecordReader()\n",
    "a, b = reader.read(filename_queue)\n",
    "\n",
    "init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "with tf.Session() as sess:\n",
    "  sess.run(init_op)\n",
    "  coord = tf.train.Coordinator()\n",
    "  threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "  a1, b1 = sess.run([a, b])\n",
    "a1 #, b1  # a1 is filename, b1 is the contents of that file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.decode_raw?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_and_decode(filename_queue):\n",
    "  \"\"\"Read TFRecords from a file and decode into single examples.\"\"\"\n",
    "  reader = tf.TFRecordReader()\n",
    "  filename, serialized_example = reader.read(filename_queue)\n",
    "  features = tf.parse_single_example(\n",
    "      serialized_example,\n",
    "      features={\n",
    "          '__INDEX': tf.FixedLenFeature([], tf.int64),\n",
    "          'slide_num': tf.FixedLenFeature([], tf.int64),\n",
    "          'tumor_score': tf.FixedLenFeature([], tf.int64),\n",
    "          'molecular_score': tf.FixedLenFeature([], tf.float32), # default for Python float is float32\n",
    "          'sample': tf.FixedLenFeature([], tf.string)\n",
    "          # TODO: DECODE SHAPE\n",
    "      })\n",
    "  index = features[\"__INDEX\"]  #tf.cast(features[\"__INDEX\"], tf.int8)\n",
    "  slide_num = features[\"slide_num\"]  #tf.cast(features[\"slide_num\"], tf.int32)\n",
    "  tumor_score = features[\"tumor_score\"]\n",
    "  molecular_score = features[\"molecular_score\"]\n",
    "  sample = tf.decode_raw(features[\"sample\"], tf.float32)  # should use the uint8 DataFrame\n",
    "  \n",
    "  return index, slide_num, tumor_score, molecular_score, sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check that first couple of examples are correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "filename = os.path.join(folder, os.listdir(folder)[0])\n",
    "filename_queue = tf.train.string_input_producer([filename])\n",
    "\n",
    "index, slide_num, tumor_score, molecular_score, sample = read_and_decode(filename_queue)\n",
    "\n",
    "init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "sess.run(init_op)\n",
    "#   coord = tf.train.Coordinator()\n",
    "threads = tf.train.start_queue_runners(sess=sess)  #, coord=coord)\n",
    "\n",
    "index1, slide_num1, tumor_score1, molecular_score1, sample1 = sess.run([index, slide_num, tumor_score, molecular_score, sample])\n",
    "index2, slide_num2, tumor_score2, molecular_score2, sample2 = sess.run([index, slide_num, tumor_score, molecular_score, sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "index1, slide_num1, tumor_score1, molecular_score1, sample1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "row1 = rows[0]\n",
    "row1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.allclose(row1.sample.toArray(), sample1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "index2, slide_num2, tumor_score2, molecular_score2, sample2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "row2 = rows[1]\n",
    "row2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.allclose(row2.sample.toArray(), sample2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert rdd partitions -> tfrecord files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "base_dir = \"data/tf\"\n",
    "train_dir = os.path.join(base_dir, \"train\")\n",
    "val_dir = os.path.join(base_dir, \"val\")\n",
    "for folder in [train_dir, val_dir]:\n",
    "  if not os.path.exists(folder):\n",
    "    os.mkdir(folder)\n",
    "train_dir, val_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df.repartition(3).foreachPartition(lambda rows: convert(rows, train_dir))\n",
    "val_df.repartition(3).foreachPartition(lambda rows: convert(rows, val_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "FOLDER = train_dir\n",
    "[os.path.join(FOLDER, f) for f in os.listdir(FOLDER)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.train.shuffle_batch?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Hyperparams & Settings\n",
    "folder=train_dir\n",
    "min_after_dequeue = 1000\n",
    "batch_size = 100  #64\n",
    "capacity = min_after_dequeue + 4 * batch_size\n",
    "num_threads = 2\n",
    "num_epochs = 100\n",
    "classes = 3\n",
    "features = 65536\n",
    "C = 1  # Number of input channels (dimensionality of input depth)\n",
    "Hin = 256  # Input height\n",
    "Win = 256  # Input width\n",
    "Hf = 3  # conv filter height\n",
    "Wf = 3  # conv filter width\n",
    "Hfp = 2  # pool filter height\n",
    "Wfp = 2  # pool filter width\n",
    "stride = 1  # conv stride\n",
    "pstride = 2  # pool stride\n",
    "pad = 1  # For same dimensions, (Hf - stride) / 2\n",
    "F1 = 32  # num conv filters in conv1\n",
    "F2 = 32  # num conv filters in conv2\n",
    "F3 = 32  # num conv filters in conv3\n",
    "N1 = 512  # num nodes in affine1\n",
    "lr = 1e-4 # learning rate\n",
    "\n",
    "# Inputs\n",
    "with tf.name_scope(\"input\") as scope:\n",
    "  # Input queues\n",
    "  filenames = [os.path.join(folder, f) for f in os.listdir(folder)]\n",
    "  filename_queue = tf.train.string_input_producer(filenames, num_epochs=num_epochs)\n",
    "  index, slide_num, tumor_score, molecular_score, sample = read_and_decode(filename_queue)\n",
    "  sample.set_shape([Hin*Win])\n",
    "  x, y_ = tf.train.shuffle_batch([sample, tumor_score], batch_size=batch_size, capacity=capacity,\n",
    "                                 min_after_dequeue=min_after_dequeue, num_threads=num_threads)\n",
    "  \n",
    "  #x = tf.placeholder(tf.float32, [None, features], name=\"x\")\n",
    "  x_image = tf.transpose(tf.reshape(x, [-1, C, Hin, Win]), perm=[0,2,3,1])  # shape (N,H,W,C)\n",
    "  #y_ = tf.placeholder(tf.int64, [None, ], name=\"y_\")\n",
    "  y_one_hot = tf.one_hot(y_-1, classes)  # or use sparse cross entropy\n",
    "  tf.summary.image(\"x\", x_image)\n",
    "  tf.summary.histogram(\"y\", y_)\n",
    "\n",
    "# Conv layer 1: conv1 -> relu1 -> pool1\n",
    "with tf.name_scope(\"conv1\") as scope:\n",
    "  W = tf.Variable(tf.random_normal([Hf, Wf, C, F1]) * np.sqrt(2.0/(Hf*Wf*C)), name=\"W\")\n",
    "  b = tf.Variable(tf.zeros([F1]), name=\"b\")\n",
    "  conv = tf.nn.conv2d(x_image, W, [1,stride,stride,1], padding=\"SAME\") + b\n",
    "  relu = tf.nn.relu(conv)\n",
    "  pool = tf.nn.max_pool(relu, ksize=[1,Hfp,Wfp,1], strides=[1,pstride,pstride,1], padding=\"SAME\")\n",
    "  tf.summary.image(\"conv1\", tf.transpose(W, [3,0,1,2]), max_outputs=F1)  # transpose to [N,H,W,C]\n",
    "\n",
    "# Conv layer 2: conv2 -> relu2 -> pool2\n",
    "with tf.name_scope(\"conv2\") as scope:\n",
    "  W = tf.Variable(tf.random_normal([Hf, Wf, F1, F2]) * np.sqrt(2.0/(Hf*Wf*F1)), name=\"W\")\n",
    "  b = tf.Variable(tf.zeros([F2]), name=\"b\")\n",
    "  conv = tf.nn.conv2d(pool, W, [1,stride,stride,1], padding=\"SAME\") + b\n",
    "  relu = tf.nn.relu(conv)\n",
    "  pool = tf.nn.max_pool(relu, ksize=[1,Hfp,Wfp,1], strides=[1,pstride,pstride,1], padding=\"SAME\")\n",
    "\n",
    "# Conv layer 3: conv3 -> relu3 -> pool3\n",
    "with tf.name_scope(\"conv3\") as scope:\n",
    "  W = tf.Variable(tf.random_normal([Hf, Wf, F2, F3]) * np.sqrt(2.0/(Hf*Wf*F2)), name=\"W\")\n",
    "  b = tf.Variable(tf.zeros([F3]), name=\"b\")\n",
    "  conv = tf.nn.conv2d(pool, W, [1,stride,stride,1], padding=\"SAME\") + b\n",
    "  relu = tf.nn.relu(conv)\n",
    "  pool = tf.nn.max_pool(relu, ksize=[1,Hfp,Wfp,1], strides=[1,pstride,pstride,1], padding=\"SAME\")\n",
    "\n",
    "# Affine layer 1:  affine1 -> relu1 -> dropout\n",
    "with tf.name_scope(\"affine1\") as scope:\n",
    "  D = int(F3*(Hin/2**3)*(Win/2**3))\n",
    "  W = tf.Variable(tf.random_normal([D,N1]) * np.sqrt(2.0/D), name=\"W\")\n",
    "  b = tf.Variable(tf.zeros([N1]), name=\"b\")\n",
    "  affine = tf.matmul(tf.reshape(pool, [-1,D]), W) + b\n",
    "  relu = tf.nn.relu(affine)\n",
    "  keep_prob = tf.placeholder(tf.float32, name=\"keep_prob\")\n",
    "  dropout = tf.nn.dropout(relu, keep_prob)\n",
    "\n",
    "# Affine layer 2:  affine2 -> softmax\n",
    "with tf.name_scope(\"affine2\") as scope:\n",
    "  W = tf.Variable(tf.random_normal([N1,classes]) * np.sqrt(2.0/N1), name=\"W\")\n",
    "  b = tf.Variable(tf.zeros([classes]), name=\"b\")\n",
    "  logits = tf.matmul(dropout, W) + b\n",
    "  probs = tf.nn.softmax(logits)\n",
    "  tf.summary.histogram(\"logits\", logits)\n",
    "  tf.summary.histogram(\"probs\", probs)\n",
    "\n",
    "# Loss\n",
    "with tf.name_scope(\"loss\") as scope:\n",
    "  cross_entropy_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, y_one_hot))\n",
    "  tf.summary.scalar(\"loss\", cross_entropy_loss)\n",
    "\n",
    "# Train\n",
    "# train_step = tf.train.GradientDescentOptimizer(lr).minimize(cross_entropy)\n",
    "train_step = tf.train.AdamOptimizer(lr).minimize(cross_entropy_loss)\n",
    "\n",
    "# Eval metrics\n",
    "with tf.name_scope(\"eval\") as scope:\n",
    "  correct_pred = tf.equal(tf.argmax(logits,1), tf.argmax(y_one_hot,1))\n",
    "  accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "  tf.summary.scalar(\"accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run `tensorboard --logdir=tf_logs --host=localhost --debug --reload_interval 5`\n",
    "with tf.Session() as sess:\n",
    "  # Summaries\n",
    "  log_dir = \"tf_logs\"\n",
    "  summary_op = tf.summary.merge_all()\n",
    "  train_writer = tf.train.SummaryWriter(log_dir + \"/train\", sess.graph)\n",
    "  val_writer = tf.train.SummaryWriter(log_dir + \"/val\")\n",
    "  \n",
    "  init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "  sess.run(init_op)\n",
    "  \n",
    "  # Start input queues\n",
    "  coord = tf.train.Coordinator()\n",
    "  threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "  \n",
    "  # Data Gen\n",
    "  #train_generator = gen_batch(train_df.rdd, 64)\n",
    "  x_val, y_val = extract_data(val_df)\n",
    "  \n",
    "  # Train\n",
    "  try:\n",
    "    i = 0\n",
    "    while not coord.should_stop():\n",
    "      _ = sess.run([train_step], feed_dict={keep_prob:0.5})\n",
    "      if i % 10 == 0:\n",
    "        # train stats\n",
    "        summary, train_acc = sess.run([summary_op, accuracy], feed_dict={keep_prob:0.5})\n",
    "        train_writer.add_summary(summary, i)\n",
    "        # val stats\n",
    "        summary, val_acc = sess.run([summary_op, accuracy], feed_dict={x: x_val, y_:y_val, keep_prob:0.5})\n",
    "        val_writer.add_summary(summary, i)\n",
    "        #train_writer.flush()  # To force write -- probably slower (usually asynchronous)\n",
    "        #val_writer.flush()  # To force write -- probably slower (usually asynchronous)\n",
    "        print(\"Iter: {}, \\t Train Accuracy: {:.4f}, \\t Val Accuracy: {:.4f}\".format(i, train_acc, val_acc))\n",
    "      i += 1\n",
    "  except tf.errors.OutOfRangeError:\n",
    "    print(\"Done training!\")\n",
    "  finally:\n",
    "    # Ask threads to stop when finished\n",
    "    coord.request_stop()\n",
    "  # Wait for all threads to finish  \n",
    "  coord.join(threads)\n",
    "  \n",
    "train_writer.flush()  # Make sure everything is written before exiting\n",
    "val_writer.flush()  # Make sure everything is written before exiting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 + Spark 1.6 + SystemML",
   "language": "python",
   "name": "pyspark3_1.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
